# Copyright 2022 OpenMined.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""DP engine to compute aggregations for sketches."""

import functools
from typing import Any, Sequence, Tuple, Union

from apache_beam import pvalue
import numpy as np
import pydp
import pipeline_dp


def aggregate_sketch_true(ops: pipeline_dp.pipeline_backend.PipelineBackend,
                          col, metric: pipeline_dp.Metrics):
    """Computes raw aggregation on sketches generated by peeker without adding noises.

    Aggregation means aggregate values group by partition_key. Both values and
    partition_key are extracted by data extractors.

    The input `col` has one entry for each unique (partition_key, privacy_id).
    They are in the format of (partition_key, partition_value, partition_count).

    partition_key: the hashed version of the current partition key
    partition_value: the per privacy id per partition_key aggregated value
    partition_count: number of partitions this privacy id contributes to

    Note, this is not a DP aggregation.
    Note, this function only supports one aggregation that needs to be either
    count or sum.

    Args:
        col: Sketches in the format of (partition_key, partition_value,
            partition_count).
        params: Specifies which metrics to compute and computation parameters.

    Returns:
        Raw aggregation metrics.
    """
    if metric == pipeline_dp.Metrics.SUM:
        aggregator_fn = sum
    elif metric == pipeline_dp.Metrics.COUNT:
        aggregator_fn = len
    else:
        raise ValueError('Aggregate sketch only supports sum or count')
    # col: (partition_key, per_user_aggregated_value, partition_count)
    col = ops.map_tuple(col, lambda pk, pval, _: (pk, pval),
                        'Drop partition count')
    # col: (partition_key, per_user_aggregated_value)
    col = ops.group_by_key(col, "Group by partition key")
    # col: (partition_key, [per_user_aggregated_value])
    col = ops.map_values(col, lambda val: aggregator_fn(val),
                         "Aggregate by partition key")
    return col


class PeekerEngine:
    """Performs aggregations for utility analysis."""

    def __init__(
            self,
            budget_accountant: pipeline_dp.budget_accounting.BudgetAccountant,
            ops: pipeline_dp.pipeline_backend.PipelineBackend):
        self._budget_accountant = budget_accountant
        self._ops = ops

    def aggregate_sketches(
        self, col, params: pipeline_dp.AggregateParams
    ) -> Sequence[Tuple[Any, Sequence[int]]]:
        """Computes approximated DP aggregation on sketches generated by peeker.

        The input `col` has one entry for each unique (partition_key, privacy_id).
        They are in the format of (partition_key, partition_value, partition_count).

        partition_key: the hashed version of the current partition key
        partition_value: the per privacy id per partition_key aggregated value
        partition_count: number of partitions this privacy id contributes to

        Note, this is not a DP aggregation. There are certain shortcuts made
        to make sure utility analysis run fast in realtime. The output is intended
        for the utility analysis. Please don't release metrics generated by this
        funciton if you want DP guarantee.

        Note, this function only supports one aggregation that needs to be either
        count or sum. This is determined by the nature of sketching.

        Args:
            col: Sketches in the format of (partition_key, partition_value,
                partition_count).
            params: Specifies which metrics to compute and computation parameters.

        Returns:
            Aggregation metrics.
        """
        if len(params.metrics) != 1 or params.metrics[0] not in [
                pipeline_dp.aggregate_params.Metrics.SUM,
                pipeline_dp.aggregate_params.Metrics.COUNT
        ]:
            raise ValueError(
                "Sketch only supports a single aggregation and it must be COUNT or SUM."
            )

        # col: (partition_key, per_user_aggregated_value, partition_count)
        combiner = pipeline_dp.combiners.create_compound_combiner(
            params, self._budget_accountant)

        col = self._ops.filter(
            col,
            functools.partial(_cross_partition_filter_fn,
                              params.max_partitions_contributed),
            "Cross partition bounding")
        # TODO: use max_contributions_per_partition for COUNT and max_sum_per_partition for SUM.
        col = self._ops.map_tuple(
            col,
            functools.partial(_per_partition_bounding,
                              params.max_contributions_per_partition),
            "Per partition bounding")
        # col: (partition_key, per_user_aggregated_value)
        col = self._ops.map_values(col, lambda x: (1, (x,)),
                                   "Convert to format of CompoundCombiner")
        # col: (partition_key, compound_accumulator)
        col = self._ops.combine_accumulators_per_key(
            col, combiner, "Aggregate by partition key")
        # col: (partition_key, aggregated_compound_accumulator)
        # Partition selection
        budget = self._budget_accountant.request_budget(
            mechanism_type=pipeline_dp.MechanismType.GENERIC)

        partition_selection_filter_fn = functools.partial(
            _partition_selection_filter_fn, budget,
            params.max_partitions_contributed)

        col = self._ops.filter(col, partition_selection_filter_fn,
                               "Filter private partitions")
        # col: (partition_key, aggregated_compound_accumulator)
        col = self._ops.map_values(col, combiner.compute_metrics,
                                   "Compute DP metrics")
        # col: (partition_key, metrics)
        return col


def _cross_partition_filter_fn(max_partitions: int, col: Tuple[Any, int,
                                                               int]) -> bool:
    _, per_user_aggregated_value, partition_count = col
    if per_user_aggregated_value <= max_partitions:
        return True
    # This is an approximation of true cross partition bounding.
    return np.random.rand() < max_partitions / partition_count


def _per_partition_bounding(max_contributions_per_partition: int, pk: Any,
                            pval: int, pcount: int) -> Tuple[Any, int]:
    del pcount  # unused
    bounded_value = min(pval, max_contributions_per_partition)
    return pk, bounded_value


def _partition_selection_filter_fn(
    budget: pipeline_dp.budget_accounting.MechanismSpec, max_partitions: int,
    row: Tuple[Any, pipeline_dp.combiners.CompoundCombiner.AccumulatorType]
) -> bool:
    """Lazily creates a partition selection strategy and uses it to determine which
    partitions to keep."""
    privacy_id_count, _ = row[1]
    partition_selection_strategy = (
        pydp.algorithms.partition_selection.
        create_truncated_geometric_partition_strategy(budget.eps, budget.delta,
                                                      max_partitions))
    return partition_selection_strategy.should_keep(privacy_id_count)
