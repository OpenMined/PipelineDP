{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F92JfP83EUrU"
   },
   "source": [
    "# Introduction to Apache Beam API\n",
    "\n",
    "This notebook contains interactive exercises which introduces to [**Apache Beam**](https://beam.apache.org/) API which are needed for doing Differentially Private (`DP`) computations. \n",
    "\n",
    "In this notebook, we will first familiarise with Apache Beam and its [Python SDK](https://beam.apache.org/documentation/sdks/python/) by walking through multiple exercises of increasing complexity. \n",
    "\n",
    "**Don't worry**: if you are not yet familiar with Apache Beam and its Python APIs, each exercise will include a _Hints_ subsection that will point you in the right direction, suggesting what you should be looking at in the documentation to solve the exercise. Then, if you are still struggling with the exercise - or simply to compare your results - you can look at the attached _Solution_.\n",
    "\n",
    "In the next notebook, we will use Apache Beam to implement a fully _differentially private_ data analysis pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "We recommend opening and running this notebook in **Google Colab** by simply clicking on the badge below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PipelineDP/blob/main/docs/tutorial_1/1_beam_introduction.ipynb)\n",
    "\n",
    "[![nbviewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/OpenMined/PipelineDP/blob/main/docs/tutorial_1/1_beam_introduction.ipynb)\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/view%20on-GitHub-blue)](https://github.com/OpenMined/PipelineDP/blob/main/docs/tutorial_1/1_beam_introduction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once in Colab, here is what you should do: \n",
    "\n",
    "1. Click \"Run in Google Colab\" link above\n",
    "2. Make a copy in Drive (File $\\mapsto$ Save a copy in Drive)\n",
    "3. Run all the cells in **Setup** area\n",
    "4. Enjoy the exercises :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XwvmAL_abu6w",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#@title Setup: Install and import `apache-beam` \n",
    "from IPython.display import clear_output\n",
    "try:\n",
    "    import apache_beam as beam\n",
    "except ImportError:\n",
    "    !pip install \"apache-beam[interactive]\"    \n",
    "    clear_output()\n",
    "finally:\n",
    "    import apache_beam as beam\n",
    "    print(\"apache_beam package available and imported correctly ✅\")\n",
    "    print(f\"Apache Beam Python SDK version: {beam.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "In this notebook we will be using the [**Netflix Prize Dataset**](https://en.wikipedia.org/wiki/Netflix_Prize) as a driver example.\n",
    "\n",
    "> The Netflix Prize was an open competition for the best collaborative filtering algorithm to predict user ratings for films, based on previous ratings without any other information about the users or films, i.e. without the users or the films being identified except by numbers assigned for the contest.\n",
    "\n",
    "(_Source_: Wikipedia)\n",
    "\n",
    "The choice of this dataset is particularly interesting from our perspective of _differentially private_ data pipelines, since this dataset was used as the reference example to present a new class of statistical de-anonymisation attacks against high-dimensional data. \n",
    "More details on these techniques, and their application to the `Netflix Prize` dataset are available in this [article](https://www.schneier.com/blog/archives/2007/12/anonymity_and_t_2.html), and in the original paper $\\Rightarrow$ [**Robust De-anonymization of Large Sparse Datasets**](https://www.cs.utexas.edu/~shmat/shmat_oak08netflix.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Netflix-Prize` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Netflix Prize dataset (`Netflix-prize`) is available on [Kaggle](https://www.kaggle.com/netflix-inc/netflix-prize-data). \n",
    "\n",
    "We will setup the Python [`kaggle`](https://pypi.org/project/kaggle/) official API to automatically download and use the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(A)** Let's first setup the `DATASET_FOLDER` where the dataset will be downloaded and saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run to set up Data and Output folders { display-mode: \"form\" }\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "NETFLIX_PRIZE_FOLDER = Path(\"netflix_prize_dataset\")\n",
    "DATASET_FOLDER = NETFLIX_PRIZE_FOLDER / \"data\"\n",
    "OUTPUT_FOLDER = NETFLIX_PRIZE_FOLDER  / \"outputs\"\n",
    "\n",
    "DATA_FILE = str(DATASET_FOLDER / \"movie_views.txt\")\n",
    "OUTFILE_TEMPLATE = str(OUTPUT_FOLDER / '{}.txt')\n",
    "\n",
    "os.makedirs(DATASET_FOLDER, exist_ok=True)\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(B)** Download the `Netflix-prize` dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run to setup `kaggle` Python API { display-mode: \"form\" }\n",
    "try:\n",
    "    import kaggle\n",
    "except ImportError:\n",
    "    !pip install kaggle\n",
    "    clear_output()\n",
    "except OSError:\n",
    "    print(\"Kaggle package is available, but credentials needs to be setup.\")\n",
    "else:\n",
    "    print(\"kaggle package imported correctly ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create and place your API token**\n",
    "\n",
    "In order to download datasets using the Kaggle API, please create and download your API token from Kaggle website:\n",
    "\n",
    "1. Log in to [Kaggle](https://www.kaggle.com/) website.\n",
    "2. Go to your **Account** (locate your profile avatar in the top-right corner of the page)\n",
    "\n",
    "![kaggle-account](images/kaggle_account.png)\n",
    "\n",
    "3. Scroll down your account page until you find the **API cell**.\n",
    "4. Click on to \"Create new API token\" button.\n",
    "\n",
    "![kaggle-api-key](images/api_key.png)\n",
    "\n",
    "This will download a file called `kaggle.json`. This file contains the credentials required to use the Kaggle APIs. \n",
    "\n",
    "5. Open this file, and include this data into the form below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup `kaggle` API credentials { display-mode: \"form\" }\n",
    "from ipywidgets import Password\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "import os\n",
    "import stat\n",
    "import json\n",
    "\n",
    "kaggle_api_credentials = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "\n",
    "API_CREDENTIALS_EXIST = os.path.exists(kaggle_api_credentials) \n",
    "\n",
    "if not API_CREDENTIALS_EXIST:\n",
    "    user = widgets.Text(\n",
    "        placeholder='Insert your Kaggle User Name!',\n",
    "        description='User Name',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "    pwd = Password(\n",
    "        description='Key',\n",
    "        placeholder='Insert key for user {}'.format(user.value)\n",
    "    )\n",
    "\n",
    "    def store_kaggle_credentials(wdgt):\n",
    "        folder = os.path.expanduser('~/.kaggle')\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        file = '{}/kaggle.json'.format(folder)\n",
    "        with open(file, 'w') as kaggle_file:\n",
    "            json.dump({\n",
    "                'username': user.value,\n",
    "                'key': wdgt.value\n",
    "            }, kaggle_file)\n",
    "\n",
    "            print(\"Credentials created in {} ✅\".format(folder))\n",
    "        os.chmod(file, stat.S_IREAD | stat.S_IWRITE)\n",
    "\n",
    "    user.on_submit(lambda wdg: display(pwd))\n",
    "    pwd.on_submit(store_kaggle_credentials)\n",
    "\n",
    "    display(user)\n",
    "else:\n",
    "    print(\"Kaggle API credentials found ✅ - All set, Ready to start!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Authenticate on Kaggle via APIs { display-mode: \"form\" }\n",
    "import kaggle\n",
    "\n",
    "kaggle.api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Search** on Kaggle for available Netflix-Prize datasets { display-mode: \"form\" }\n",
    "!kaggle datasets list -s \"netflix-prize\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top on the list is the dataset we are looking for: `netflix-inc/netflix-prize-data`.\n",
    "\n",
    "The dimension of the archive is `638 MB`, which means it may take some time to download and uncompress. (⚠️ **this may take sometime, depending on your internet connection**) Please keep this in mind while you will run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to initiate the download { display-mode: \"form\" }\n",
    "if not len(os.listdir(DATASET_FOLDER)) or not os.path.exists(DATA_FILE):\n",
    "    # Checking here to avoid download multiple times in case of notebook re-run\n",
    "    kaggle.api.dataset_download_files(\"netflix-inc/netflix-prize-data\", \n",
    "                                      path=DATASET_FOLDER, unzip=True, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Inspect** the content of the Data folder { display-mode: \"form\" }\n",
    "os.listdir(DATASET_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Netflix-prize` data files\n",
    "\n",
    "Before getting into the actual processing of this dataset with **Apache Beam**, let's briefly describe the actual data files included in the Kaggle archive. \n",
    "\n",
    "The dataset is organised into multiple files, accounting for over `100` million ratings from `480` thousand\n",
    "randomly-chosen, anonymous Netflix customers, over `17` thousand movie titles. \n",
    "\n",
    "- The `probe.txt` and the `qualitfying.txt` files are needed for the submission on Kaggle, so discarded as not interesting here.\n",
    "\n",
    "- Movie information (i.e. _metadata_) are contained in the `movie_titles.txt` in the following format:\n",
    "\n",
    "```\n",
    "MovieID,YearOfRelease,Title\n",
    "\n",
    "- MovieID is a general (randomised) ID for movies, rangin from 1 to 17,770.\n",
    "- YearOfRelease can range from 1,890 to 2,005.\n",
    "- Title (in English) is the Netflix title.\n",
    "```\n",
    "\n",
    "- The various `combined_data_*.txt` contain the actual data in the following format:\n",
    "\n",
    "```\n",
    "MovieID:\n",
    "CustomerID,Rating,Date\n",
    "CustomerID,Rating,Date\n",
    "....\n",
    "\n",
    "- MovieIDs matches those in the list of \"movie_titles.txt\".\n",
    "- CustomerIDs range from 1 to 2,649,429, with gaps. There are 480,189 users.\n",
    "- Ratings are on a five star (integral) scale from 1 to 5.\n",
    "- Dates have the format YYYY-MM-DD.\n",
    "```\n",
    "\n",
    "A more thorough description of each of those files is available in the [`README`](netflix_prize_dataset/data/README) file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing our dataset \n",
    "\n",
    "The size of the whole dataset ($\\approxeq$ `2GB` if joining together all the `combined_data_` files) doesn't allow to process the entire collection as quickly and interactive as we would expect into an interactive environment like Jupyter notebooks. This is still true despite all the boosting trick we will be setting up on `beam` later, nonetheless. \n",
    "\n",
    "For this reason, we will limit our analysis to a reduced version of the dataset. In the next two cells, we will quickly gather some statistics on the size of each data file (in terms of `number of lines`), and we will interactively choose how many of these lines we will retain in our working version.\n",
    "\n",
    "**Note**: Without any loss of generality, we will use `combined_data_1.txt` as for the reference data file. As you will soon realise, it won't make much difference. However, feel free to experiment by changing the reference file, or even by joining all of them together, if you feel your computing environment is able to handle this dataset in a reasonable amount of time (_more on this later_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Let's see how many lines each data file contains { display-mode: \"form\" }\n",
    "data_files = filter(lambda f: f.startswith(\"combined_data_\"), os.listdir(DATASET_FOLDER))\n",
    "lines_count = 0\n",
    "for data_file in sorted(data_files):\n",
    "    file_path = DATASET_FOLDER / data_file\n",
    "    wc_l = !wc -l $file_path\n",
    "    lines, _ = wc_l[0].strip().split()\n",
    "    lines_count += int(lines)\n",
    "    print(f\"{data_file}:  {int(lines):,}\")\n",
    "print(f\"Total Lines Count  : {lines_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these statistics, we can immediately notice that each data file roughly contains the same number of lines, which means that the amout of time required to process each one of them will be more or less the same.\n",
    "\n",
    "However, even just limiting our analysis to one single file (i.e. `>20M` lines) might be prohibitive, and indeed beyond of the scope of this tutorial. \n",
    "\n",
    "The **main goal** of the tutorial is to demostrate the ability of `apache_beam` to scale up the computation on large datasets. For real scalability however, we might also need decent computation capacity (_and we generally don't, if running this notebook on our laptop_).\n",
    "\n",
    "Therefore, let's decide how many lines we want our _reduced_ dataset to keep! \n",
    "\n",
    "After a few testing on the performance, we will be retaining just `500K` lines by **default** as this will result in a _few seconds_ execution on Google Colab (on the default VM instances with just _2 cores_ ).\n",
    "\n",
    "➡️ Feel free to increase this baseline if you are up to process a much larger dataset. For example, when running this notebook on my laptop (`mac OS` with `8 cores`) I was able to get `10 secs` execution time for `1M` lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup the size (in lines) of the (_reduced_) Netflix-prize Dataset {display-mode: \"form\"}\n",
    "\n",
    "#@markdown Different sizes (i.e. `#lines`) will generally scale up linearly with the computation time.\n",
    "\n",
    "# Note: this piece of code is a bit weird when read into a notebook, but it makes much\n",
    "# more sense if run into Google Colab as it uses the new Form feature\n",
    "DATASET_SIZE = \"500K lines\" #@param [\"10K lines\", \"100K lines\", \"500K lines\", \"1M lines\", \"5M lines\", \"10M lines\", \"15M lines\", \"20M lines\", \"Full Dataset\"] {type: \"string\"}\n",
    "\n",
    "if DATASET_SIZE == \"Full Dataset\":\n",
    "    DS_LINES = 24058263  # whole combined_data_1.txt file\n",
    "else:\n",
    "    DS_LINES = int(DATASET_SIZE.replace(\" lines\", \"\").replace(\"K\", \"000\").replace(\"M\", \"000000\"))\n",
    "\n",
    "print(\"Selected Number of Lines: {:,}\".format(DS_LINES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Create** the reference data file `movie_views.txt` { display-mode: \"form\" }\n",
    "\n",
    "!head -n $DS_LINES $DATASET_FOLDER/combined_data_1.txt > $DATA_FILE \n",
    "!du -h $DATA_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **WARNING**\n",
    "\n",
    "While preparing this notebook, I realised that the `movie_titles.csv` file is not originally encoded as `utf8`, but the `latin-1` which is unfortunately **not supported** in `apache_beam`.\n",
    "\n",
    "Therefore, we are going to convert file encoding to be compatible with `apache_beam` for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $DATASET_FOLDER/movie_titles.csv $DATASET_FOLDER/movie_titles_latin1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATASET_FOLDER/\"movie_titles_latin1.csv\", encoding=\"latin-1\") as mtl, \\\n",
    "open(DATASET_FOLDER/\"movie_titles.csv\", \"wb\") as mtut8:\n",
    "    ll = mtl.read()\n",
    "    mtut8.write(ll.encode(\"utf-8\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Well done! \n",
    "\n",
    "The setup of the `Netflix-prize` dataset is now complete! Now it is time to start setting up our execution framework based on **Apache Beam**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Execution Framework\n",
    "\n",
    "In this section we will define the core main components that will be used throughout the exercises. These components will be based on **Apache Beam**, which consitutes the reference computational framework, and provides the building blocks to define our data workflows.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apache_beam` in a Nutshell\n",
    "\n",
    "Apache Beam adheres to a programming model which is centred around a few key elements:\n",
    "\n",
    "* `Pipeline`: determines the **main** _data processing workflow_ as defined as a _sequence_ of **operations** on the data. This sequence is internally encoded as a directed acyclic graph in which nodes and arcs correspond to instances of `PCollection` and `PTransform`, respectively. \n",
    "\n",
    "\n",
    "- `PCollection`: represents a generic (**parallel**, `P`) data collection that can be either _bounded_ or _unbounded_ in size.\n",
    "\n",
    "\n",
    "- `PTransform`: encodes a single (transform) operation. Each single `PTransform` accepts a `PCollection` in input, and it is expected to return a new `PCollection`. Beam provides some [**core**](https://beam.apache.org/documentation/programming-guide/#core-beam-transforms) transformers (e.g. [`ParDo`](https://beam.apache.org/documentation/programming-guide/#pardo), [`Flatten`](https://beam.apache.org/documentation/programming-guide/#flatten), and [`Partition`](https://beam.apache.org/documentation/programming-guide/#partition)) which correspondo to specific processing paradigms.\n",
    "\n",
    "\n",
    "- `Runner`: (or `PipelineRunner`) determines the computational environment in which the whole data processing will be executed. \n",
    "\n",
    "Finally, pipelines I/O operations are supported by the `apache_beam` framework as core primitives (i.e. `PTransform`) to `Read` and `Write` from and to external sources.\n",
    "\n",
    "$\\Rightarrow$ More information on the Basics of [Beam Model](https://beam.apache.org/documentation/basics/) and on [Pipeline I/O](https://beam.apache.org/documentation/programming-guide/#pipeline-io) are available in the online documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Execution Modules on `Netflix-Prize` dataset\n",
    "\n",
    "Despite of its simplicity, the reference programming model in Apache Beam accounts for one very important feature: **composability**. In fact, it is possible to either re-use (part of) a `Pipeline` within other and more complex workflows, or simply chaining multiple pipelines together. \n",
    "\n",
    "With specific reference to the `Netflix-prize` dataset, the general default workflow is composed by the following steps:\n",
    "\n",
    "1. **[S1]** Read from the `DATA_FILE`, and generate the reference `PCollection`\n",
    "    - `S1.1` A proper **data abstraction** is necessary to encapsulate the information for each _movie view_;\n",
    "    - `S1.2` Data I/O will be performed in a parallel fashion (i.e. `ParDo`), leveraging on one of the most important feature of Apache Beam: _Data Parallelization_. \n",
    "\n",
    "\n",
    "2. **[S2]** Pass on the generated `PCollection` to the core (_abstract_) processing module \n",
    "    - `S2.1` Each core module will be expected as a _callable_ (implemented in the following exercises) to return the final `PCollection` to store and save (see `S3`).\n",
    "\n",
    "3. **[S3]** Write the results to the corresponding `OUTPUT_FILE`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`S1.1`** Data Abstraction\n",
    "\n",
    "To encapsulate the information for each _movie view_, we will make use of Python 3.7+ `dataclass` (see [doc](https://docs.python.org/3.8/library/dataclasses.html)). \n",
    "\n",
    "Our reference `MovieView` dataclass model is very simple, and will represent the atomic **data object** included in the `netflix-prize` `PCollection`. \n",
    "\n",
    "To simplify objects instantiation, the `date_stamp` init attribute (i.e. [`InitVar`](https://docs.python.org/3.8/library/dataclasses.html#init-only-variables)), will be later converted with proper data type and format in the `date` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, InitVar\n",
    "from datetime import datetime\n",
    "\n",
    "@dataclass\n",
    "class MovieView:\n",
    "    user_id: int\n",
    "    movie_id: int\n",
    "    rating: int\n",
    "    date_stamp: InitVar[str]\n",
    "    date: datetime.date = None\n",
    "    \n",
    "    def __post_init__(self, date_stamp):\n",
    "        if self.date is None and date_stamp is not None:\n",
    "            self.date = datetime.strptime(date_stamp, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the `MovieTitle` data class will encapsulate the `metadata` information extracted per single movie (see `movie_titles.txt` data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MovieTitle:\n",
    "    movie_id: int\n",
    "    year: int\n",
    "    name: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Python Type Safety and Custom objects\n",
    "\n",
    "The Apache Beam SDK for Python uses type hints during pipeline construction and runtime to try to emulate the correctness guarantees achieved by true static typing. Additionally, using type hints lays some groundwork that allows the backend service to perform efficient type deduction and registration of `Coder` objects.\n",
    "\n",
    "Since in this tutorial we want to handle our own custom **data object**, we also need to implement our custom (_yet simple_) `beam.coders.Coder` to allow for a correct object _serialisation_.\n",
    "\n",
    "**Note**: This step is particularly crucial during the execution (_in parallel_) of the Pipeline, to avoid `apache_beam` to fallback to the default `Serializer` (i.e. `Coder`) based in `pickle`.\n",
    "More information on **Python Type Safety** in Apache Beam pipelines is available in the official [documentation](https://beam.apache.org/documentation/sdks/python-type-safety/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieViewCoder(beam.coders.Coder):\n",
    "    \"\"\"Beam Coder Serialiser for custom MovieView Data Object\"\"\"\n",
    "    def encode(self, view):\n",
    "        return (f\"{view.user_id}:{view.movie_id}:{view.rating}:{str(view.date.date())}\").encode(\"utf-8\")\n",
    "\n",
    "    def decode(self, s):\n",
    "        return MovieView(*s.decode(\"utf-8\").split(':'))\n",
    "\n",
    "    def is_deterministic(self):\n",
    "        return True\n",
    "    \n",
    "\n",
    "class MovieTitleCoder(beam.coders.Coder):\n",
    "    \"\"\"Beam Coder Serialiser for custom MovieTitle Data Object\"\"\"\n",
    "    def encode(self, title):\n",
    "        return (f\"{title.movie_id}--{title.year}--{title.name}\").encode(\"utf-8\")\n",
    "    \n",
    "    def decode(self, s):\n",
    "        return MovieTitle(*s.decode(\"utf-8\").split('--'))\n",
    "    \n",
    "    def is_deterministic(self):\n",
    "        return True\n",
    "\n",
    "\n",
    "# Once defined, we can register the new Coders\n",
    "beam.coders.registry.register_coder(MovieView, MovieViewCoder)\n",
    "beam.coders.registry.register_coder(MovieTitle, MovieTitleCoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Workaround `multiprocessing` issues with namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **Note**: As a workaround to enable _parallel computation_ in the notebook via Python `multiprocessing` and `beam.DoFn` (see next section), the dataclass and its coder will be saved into an external module (i.e. `movie_view.py`) for later use via **direct import**. This will guarantee a proper object serialisation, while also adhering to **Apache Beam** requirements. See [Requirements for writing user code for beam transforms](https://beam.apache.org/documentation/programming-guide/#requirements-for-writing-user-code-for-beam-transforms) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "echo -e \"\n",
    "from dataclasses import dataclass, InitVar\n",
    "from datetime import datetime\n",
    "import apache_beam as beam\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MovieView:\n",
    "    user_id: int\n",
    "    movie_id: int\n",
    "    rating: int\n",
    "    date_stamp: InitVar[str]\n",
    "    date: datetime.date = None\n",
    "    \n",
    "    def __post_init__(self, date_stamp):\n",
    "        if self.date is None and date_stamp is not None:\n",
    "            self.date = datetime.strptime(date_stamp, '%Y-%m-%d')\n",
    "            \n",
    "\n",
    "@dataclass\n",
    "class MovieTitle:\n",
    "    movie_id: int\n",
    "    year: int\n",
    "    name: str\n",
    "            \n",
    "\n",
    "class MovieViewCoder(beam.coders.Coder):\n",
    "    \\\"\\\"\\\"Beam Coder Serialiser for our custom MovieView Data Object\\\"\\\"\\\"\n",
    "    def encode(self, view):\n",
    "        return (f'{view.user_id}:{view.movie_id}:{view.rating}:{str(view.date.date())}').encode('utf-8')\n",
    "\n",
    "    def decode(self, s):\n",
    "        return MovieView(*s.decode('utf-8').split(':'))\n",
    "\n",
    "    def is_deterministic(self):\n",
    "        return True\n",
    "\n",
    "class MovieTitleCoder(beam.coders.Coder):\n",
    "    \\\"\\\"\\\"Beam Coder Serialiser for custom MovieTitle Data Object\\\"\\\"\\\"\n",
    "    def encode(self, title):\n",
    "        return (f'{title.movie_id}:{title.year}:{title.name}').encode('utf-8')\n",
    "    \n",
    "    def decode(self, s):\n",
    "        return MovieTitle(*s.decode('utf-8').split(':'))\n",
    "    \n",
    "    def is_deterministic(self):\n",
    "        return True\n",
    "\n",
    "\n",
    "# Once defined, we can register the new Coders\n",
    "beam.coders.registry.register_coder(MovieView, MovieViewCoder)\n",
    "beam.coders.registry.register_coder(MovieTitle, MovieTitleCoder)\n",
    "\" > netflix_movies.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Verify** that import from movie_view module works properly { display-mode: \"form\" } \n",
    "try: \n",
    "    from netflix_movies import MovieView, MovieTitle\n",
    "except ImportError:\n",
    "    print(\"netflix_movies module cannot be imported. Please check!\")\n",
    "else:\n",
    "    print(\"MovieView and MovieTitle dataclasses have been successfully imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`S1.2`** `beam.ParDo` handler\n",
    "\n",
    "The [`beam.ParDo`](https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.core.html#apache_beam.transforms.core.ParDo) processing paradigm is very similar to the `Map` phase of a `Map`/`Shuffle`/`Reduce`-style algorithm. \n",
    "\n",
    "The [`ParDo`](https://beam.apache.org/documentation/transforms/python/elementwise/pardo/) transformer is executed in parallel elementwise to the `PCollection`, via custom [`beam.DoFn`](https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.core.html#apache_beam.transforms.core.DoFn) callable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@beam.typehints.with_output_types(MovieView)  # type-hint annotation for the output\n",
    "class ParseMovieViews(beam.DoFn):\n",
    "    \"\"\"ParDo main execution core. Each line of the DATA_FILE will be processed, \n",
    "    and an instance of MovieView will be created and returned, accordingly.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.movie_id: int = -1\n",
    "    \n",
    "    def process(self, line: str):\n",
    "        from netflix_movies import MovieView\n",
    "        line = line.strip()  # get rid of any useless tabulation\n",
    "        if not line:\n",
    "            return  # Skip Line\n",
    "        if line.endswith(\":\"):\n",
    "            movie_id, _ = line.split(\":\")\n",
    "            self.movie_id = int(movie_id)\n",
    "            return  # Automatically skip this line\n",
    "        user_id, rating, date_stamp, *_ = line.split(',')\n",
    "        user_id, rating = int(user_id), int(rating)\n",
    "        yield MovieView(user_id, self.movie_id, rating, date_stamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ParseMovieViews` callable will be used to generate the initial reference `PCollection` of `MovieView` instances for the `Netflix-prize` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now work out a solution in a very similar fashion to parse _movie_titles_ as extracted from the `movie_titles.txt` dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@beam.typehints.with_output_types(MovieTitle)  # type-hint annotation for the output\n",
    "class ParseMovieTitles(beam.DoFn):\n",
    "    \"\"\"ParDo main execution core. Each line of the DATA_FILE will be processed, \n",
    "    and an instance of MovieTitle will be created and returned, accordingly.\"\"\"\n",
    "    \n",
    "    def process(self, line: str):\n",
    "        from netflix_movies import MovieTitle\n",
    "        line = line.strip()  # get rid of any useless tabulation\n",
    "        if not line:\n",
    "            return  # Skip Line\n",
    "        movie_id, year, *name = line.split(',')  # title may contain a comma as well\n",
    "        movie_id, name = int(movie_id), \",\".join(name)\n",
    "        year = -1 if year == \"NULL\" else int(year)\n",
    "        yield MovieTitle(movie_id, year, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`S2.1`** The core `beam.Pipeline`\n",
    "\n",
    "Now it is finally the time to create the whole `beam.Pipeline` infrastructure, that will allow pluggable callables to be used (as implemented in the following Exercises).\n",
    "The Pipeline will be also configured to leverage as much as possible parallel execution in a way that we could circumvent the Python [**GIL**](https://wiki.python.org/moin/GlobalInterpreterLock)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating PCollections**\n",
    "\n",
    "The first step will be to create a function that returns the reference `PCollection` we want to deal with. Having two separate **data objects** (i.e. `MovieView` and `MovieTitle`), we might assume we will be needing two separate functions to gather instances of either type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netflix_movie_views_collection(p: beam.Pipeline, data_file: str = DATA_FILE) -> beam.PCollection[MovieView]:\n",
    "    \"\"\"Generate the initial (Parallel) Collection of \n",
    "    `MovieView` objects as extracted from the input `data_file`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p: beam.Pipeline\n",
    "        The current reference pipeline object to run\n",
    "    data_file: str (default: DATASET_FOLDER/movie_views.txt)\n",
    "        Path (as str, as expected by `apache_beam`) to the reference dataset file\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    beam.PCollection: The (parallel) collection of `MovieView` instances, ready for further processing.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        p\n",
    "        | \"Read MovieViews from Data File\" >> beam.io.ReadFromText(data_file)\n",
    "        | \"Parse into MovieView objects\" >> beam.ParDo(ParseMovieViews()).with_output_types(MovieView)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netflix_movie_title_collection(p: beam.Pipeline, data_file: str = str(DATASET_FOLDER/\"movie_titles.csv\")) -> beam.PCollection[MovieTitle]:\n",
    "    \"\"\"Generate the initial (Parallel) Collection of \n",
    "    `MovieView` objects as extracted from the input `data_file`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p: beam.Pipeline\n",
    "        The current reference pipeline object to run\n",
    "    data_file: str (default: DATASET_FOLDER/movie_views.txt)\n",
    "        Path (as str, as expected by `apache_beam`) to the reference dataset file\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    beam.PCollection: The (parallel) collection of `MovieTitle` instances, ready for further processing.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        p\n",
    "        | \"Read MOvieTitle from Data File\" >> beam.io.ReadFromText(data_file)\n",
    "        | \"Parse into MovieTitle objects\" >> beam.ParDo(ParseMovieTitles()).with_output_types(MovieTitle)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Pipeline**\n",
    "\n",
    "The last component of our framework will be devoted the actual _execution_ of the pipeline. In particular, we will define the _core_ `run_pipeline` function which will accept the following three parameters:\n",
    "\n",
    "- `pipeline_fn`: the _pluggable_ `PipelineModule` to execute;\n",
    "\n",
    "\n",
    "- `pcollection_fn`: the selected callable `PCollectionFun` to call to gather the target data objects `PCollection` (either `netflix_movie_views_collection` or `netflix_movie_title_collection`)\n",
    "\n",
    "\n",
    "- `running_mode`: execution mode to be passed on as `PipelineOption`. Accepted values are (I) `multi_processing` (default); (II) `multi_threading`; (III) `in_memory`. More information on the pipeline options are available in the official [documentation](https://beam.apache.org/documentation/patterns/pipeline-options/);\n",
    "\n",
    "\n",
    "\n",
    "- `data_file`: string matching the path to the input `data_file` (to be passed to the `netflix_movie_views_collection` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A PipelineModule is defined as a Callable that accepts any parameter\n",
    "# and returns a beam.PCollection` instance\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "PipelineModule = Callable[[beam.PCollection], beam.PCollection]\n",
    "PCollectionFun = Callable[[beam.Pipeline, str], beam.PCollection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Check** Python multiprocessing settings { display-mode: \"form\" }\n",
    "import multiprocessing as mp\n",
    "\n",
    "print(\"Python Multiprocessing Start method: \", mp.get_start_method())\n",
    "print(\"Multiprocessing available Cores: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from apache_beam.runners import DirectRunner\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "def run_pipeline(pipeline_fn: PipelineModule, \n",
    "                 pcollection_fn: PCollectionFun = netflix_movie_views_collection,\n",
    "                 running_mode: str = \"multi_processing\", \n",
    "                 data_file: str = DATA_FILE):\n",
    "    start_time = time.time()\n",
    "    fn_name = pipeline_fn.__name__\n",
    "    outfile = str(OUTFILE_TEMPLATE).format(fn_name)  # output logfile\n",
    "\n",
    "    # Pipeline Options\n",
    "    pipeline_options = PipelineOptions.from_dictionary(\n",
    "        {\n",
    "            \"direct_num_workers\": 0,  # setting parallelism to CPU count - alternatively: mp.cpu_count() - 1\n",
    "            \"direct_running_mode\": running_mode,  # work around GIL\n",
    "            \"job-server-timeout\": 65536,  # used to avoid potential timeout interruptions of workers\n",
    "        }\n",
    "    )\n",
    "    with beam.Pipeline(runner=DirectRunner(), options=pipeline_options) as p:\n",
    "        views_coll = pcollection_fn(p, data_file=data_file)\n",
    "        pipeline_fn(views_coll) | \"Write result\" >> beam.io.WriteToText(\n",
    "            outfile, shard_name_template=\"\"\n",
    "        )\n",
    "    # Timing\n",
    "    elapsed_time = time.time() - start_time\n",
    "    hours, remainder = divmod(elapsed_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f\"Elapsed time in {fn_name} is {hours:2.0f}H {minutes:2.0f}m {seconds:2.0f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ A few notes on the `PipelineOptions` used:\n",
    "\n",
    "- `running_mode` is set to `multi_processing` by default as to work around the Python GIL, and so leverage on a full parallelization capacity in Python environment. However, when running on **Google Colab**, this won't make much of a difference as per execution time since only `2 cores` are available. Therefore, either `in_memory` or `multi_processing` will have more or less the same execution time. \n",
    "\n",
    "\n",
    "- `job-server-timeout` is set to `65536` to avoid any potential timeout in the backend that may happen. Please feel free to increase this number considerably when running on larger dataset. (See [`grpc:16038`](https://github.com/grpc/grpc/issues/16038))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuPWswzQyBF0"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "Each exercise it to write an Apache Beam pipeline, which does some transformations on a part of Netflix prize dataset data. \n",
    "\n",
    "The completion of the exercises requires **additional research** on usage of Beam API with a search engine or looking at the offcial [API reference](https://beam.apache.org/documentation/sdks/python/). \n",
    "\n",
    "Let's start by looking at first lines of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10 $DATA_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BPomLO8zy4t"
   },
   "source": [
    "##### RECAP\n",
    "\n",
    "The exercise framework function **`run_pipeline`** executes a user defined pipeline function.\n",
    "\n",
    "The output of the pipeline is written in a file `outputs/(pipeline_fn name)`.\n",
    "\n",
    "\n",
    "**Note**:\n",
    "1. Each exercise has a solution, to test and verify your own implementation; \n",
    "\n",
    "2. In all exercises elements of the input collection are `MovieView` objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhPG9AAiFGns"
   },
   "source": [
    "Let's start with an example: \n",
    "\n",
    "**A**) Write a pipeline function for computing number of records in the dataset. \n",
    "The example code is in the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.transforms import combiners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of records in the dataset\n",
    "\n",
    "# The exercise framework calls this function with |views| representing all MovieViews in the dataset.\n",
    "# |views| has type PCollection, which is a parrallel collection.\n",
    "# Beam pipelines consist of applying PTransforms (i.e. parallel transformations) on PCollections\n",
    "def count_all_views(views: beam.PCollection): \n",
    "   # This is an example of applying PTransform\n",
    "   # Beam uses | (pipe) syntax:\n",
    "   # new_p_collection = p_collection | \"Optional comment\" >> PTransform\n",
    "   return views | \"Count all views\" >> combiners.Count.Globally().with_input_types(MovieView)\n",
    "\n",
    "\n",
    "run_pipeline(count_all_views)  \n",
    "\n",
    "print(\"\\nFirst lines of the output file:\")\n",
    "!head -1 $OUTPUT_FOLDER/\"count_all_views.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJN_Icn2DDAi"
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "Write a pipeline function for computing **distinct** movies IDs in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9aSeuIrMD6-z"
   },
   "outputs": [],
   "source": [
    "def distinct_movies_ids(views):\n",
    "    # TODO: write your code here\n",
    "    pass\n",
    "\n",
    "run_pipeline(distinct_movies_ids)\n",
    "\n",
    "print(\"\\Content of the output file:\")\n",
    "!cat $OUTPUT_FOLDER/\"distinct_movies_ids.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Sl4XO84sEJZy"
   },
   "outputs": [],
   "source": [
    "#@title Hint (double click to open)\n",
    "\n",
    "#@markdown Search for `beam.Map` and `beam.Distinct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XtyCCoHJDCUr",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#@title Solution (click on the arrow button to run, double click to open)\n",
    "def distinct_movies_ids(views):\n",
    "    return (\n",
    "        views \n",
    "        | beam.Map(lambda mv: mv.movie_id)\n",
    "        | beam.Distinct())\n",
    "\n",
    "run_pipeline(distinct_movies_ids)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!cat $OUTPUT_FOLDER/\"distinct_movies_ids.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnAQT1kzEfmL"
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "Write a pipeline function for computing the **number** of distinct movies ids in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jqgHgbfE1CD"
   },
   "outputs": [],
   "source": [
    "def number_distinct_movies_ids(views):\n",
    "    # TODO: write your code here\n",
    "    \n",
    "run_pipeline(number_distinct_movies_ids)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!cat $OUTPUT_FOLDER/\"number_distinct_movies_ids.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Hint\n",
    "\n",
    "# You could join the functions from the previous two exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "KVJ1EqOMFaWj"
   },
   "outputs": [],
   "source": [
    "#@title Solution (click on the arrow button to run, double click to open)\n",
    "def number_distinct_movies_ids(views):\n",
    "    return (\n",
    "        distinct_movies_ids(views) \n",
    "        | combiners.Count.Globally())\n",
    "\n",
    "run_pipeline(number_distinct_movies_ids)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!cat $OUTPUT_FOLDER/\"number_distinct_movies_ids.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEg0IdtM-USW"
   },
   "source": [
    "## Exercise 3\n",
    "\n",
    "Write a pipeline which computes the number of views per movie \n",
    "i.e. it outputs `PCollection(movie_id, number of views)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5F4sUYId-tKk"
   },
   "outputs": [],
   "source": [
    "def views_count_per_movie(views):\n",
    "    # TODO: write your code\n",
    "\n",
    "run_pipeline(views_count_per_movie)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!cat $OUTPUT_FOLDER/\"views_count_per_movie.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "vjfFjkVm_s5_"
   },
   "outputs": [],
   "source": [
    "#@title Hint (double click to open)\n",
    "#@markdown Search for `beam.combiners.Count.PerElement` or beam.combiners.Count.PerKey`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "f5dIxHKt-u-d"
   },
   "outputs": [],
   "source": [
    "#@title Solution\n",
    "def views_count_per_movie(views):\n",
    "    return ( \n",
    "        views \n",
    "        | beam.Map(lambda mv: mv.movie_id)\n",
    "        | \"Count movies\" >> combiners.Count.PerElement())\n",
    "\n",
    "run_pipeline(views_count_per_movie)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!cat $OUTPUT_FOLDER/\"views_count_per_movie.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gizp5nvB_P_X"
   },
   "source": [
    "## Exercise 4\n",
    "\n",
    "Write a pipeline which computes the _average_ rating per movie, i.e. it outputs `PCollection(movie_id, average rating)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZP12hCn_1Pu"
   },
   "outputs": [],
   "source": [
    "def mean_rating_per_movie(views):\n",
    "    # TODO: write your code here\n",
    "\n",
    "run_pipeline(mean_rating_per_movie)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!cat \"outputs/mean_rating_per_movie.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "iZUtNUwg_wfm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#@title Solution\n",
    "\n",
    "def mean_rating_per_movie(views):\n",
    "    # 2 element tuples then tuples are treated as (key, value) in PCollections\n",
    "    return (\n",
    "        views \n",
    "        | beam.Map(lambda mv: (mv.movie_id, mv.rating))\n",
    "        | combiners.Mean.PerKey())\n",
    "\n",
    "run_pipeline(mean_rating_per_movie)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!cat $OUTPUT_FOLDER/\"mean_rating_per_movie.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Rahkq8mAoyk"
   },
   "source": [
    "## Exercise 5\n",
    "\n",
    "Write a pipeline which bounds number of views per user by `|max_views|`, it outputs `PCollection(MovieView)`, such that for each `user_id` there are not more than `|max_views|` records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SU5CKbs9BT_3"
   },
   "outputs": [],
   "source": [
    "def bound_number_of_views_per_user(views, max_views=2):\n",
    "    # TODO: write your code here\n",
    "\n",
    "run_pipeline(bound_number_of_views_per_user)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!head -5 $OUTPUT_FOLDER/\"bound_number_of_views_per_user.txt\"\n",
    "\n",
    "print(\"\\nNumber of lines (the correct number is 326037 for max_views=2 and 500M lines data file):\")\n",
    "!wc -l $OUTPUT_FOLDER/\"bound_number_of_views_per_user.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "g8E6o2PgCpOH"
   },
   "outputs": [],
   "source": [
    "#@title Hint\n",
    "#@markdown Search for `combiners.Sample` and `beam.FlatMap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "A4HuEH_lA5y2"
   },
   "outputs": [],
   "source": [
    "#@title Solution\n",
    "def bound_number_of_views_per_user(views,max_views=2):\n",
    "    return (\n",
    "        views \n",
    "        | beam.Map(lambda mv: (mv.user_id, mv))\n",
    "        # user_id -> list of MovieView\n",
    "        | combiners.Sample.FixedSizePerKey(max_views)\n",
    "        | \"Remove keys\" >> beam.Values()\n",
    "        | \"Unnest lists\" >> beam.FlatMap(lambda x: x))\n",
    "\n",
    "run_pipeline(bound_number_of_views_per_user)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!head -5 $OUTPUT_FOLDER/\"bound_number_of_views_per_user.txt\"\n",
    "print(\"\\nNumber of lines:\")\n",
    "!wc -l $OUTPUT_FOLDER/\"bound_number_of_views_per_user.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgNEnrKeHZUo"
   },
   "source": [
    "## Exercise 6\n",
    "Write a pipeline which computes rating histogram per movie, i.e. \n",
    "it outputs `PCollection((movie_id, histogram_tuple))`, where `histogram_tuple` is \n",
    "the `5` elements tuple, which contains the number of views with corresponding rating.\n",
    "\n",
    "This exercise is for learning how to create custom combiners (look for `beam.CombineFn`).\n",
    "\n",
    "**Note**: this exercise is a bit more difficult than previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmaNHtd5G1Xg"
   },
   "outputs": [],
   "source": [
    "def movie_rating_histogram(views):\n",
    "    # TODO: write your code here\n",
    "\n",
    "run_pipeline(movie_rating_histogram)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!head -10 $OUTPUT_FOLDER/\"movie_rating_histogram.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Hint\n",
    "\n",
    "#@markdown Have a look at `beam.CombineFn` `PTransform`, and `CombinePerKey`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lEZNOSX3C-xx"
   },
   "outputs": [],
   "source": [
    "#@title Solution\n",
    "class SumTuplesCombiner(beam.CombineFn):\n",
    "    def __init__(self, ndim):\n",
    "        self.ndim = ndim\n",
    "\n",
    "    def create_accumulator(self):\n",
    "        return [0] * self.ndim\n",
    "\n",
    "    def add_input(self, accumulator, element):\n",
    "        for i, b in enumerate(element):\n",
    "            accumulator[i] += b\n",
    "        return accumulator\n",
    "\n",
    "    def merge_accumulators(self, accumulators):\n",
    "        res = self.create_accumulator()\n",
    "        for a in accumulators:\n",
    "            self.add_input(res, a)\n",
    "        return res\n",
    "\n",
    "    def extract_output(self, accumulator):\n",
    "        return accumulator\n",
    "\n",
    "\n",
    "def movie_rating_histogram(views):\n",
    "    def one_hot(rating):\n",
    "        return (rating == 1, rating == 2, rating == 3, rating == 4, rating == 5)\n",
    "\n",
    "    return (\n",
    "        views \n",
    "        | beam.Map(lambda mv: (mv.movie_id, one_hot(mv.rating)))\n",
    "        | beam.CombinePerKey(SumTuplesCombiner(ndim=5)))\n",
    "\n",
    "run_pipeline(movie_rating_histogram)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!head -10 $OUTPUT_FOLDER/\"movie_rating_histogram.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTCuMUNMHcVw"
   },
   "source": [
    "## Exercise 7\n",
    "\n",
    "This exercise has **2 parts**, the first one is about reading from files, the second is about using `join`.\n",
    "\n",
    "In this exercise we will work with `MovieTitle`. The next code cell contains data class definition and code for parsing `MovieTitle` from string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJit3uvPKiKa"
   },
   "source": [
    "### Exercise 7a\n",
    "\n",
    "Read and parse a movie titles file and output movie **names**. It should output `PCollection(str)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aw1p74fuNrhy"
   },
   "outputs": [],
   "source": [
    "def read_movie_names(movie_titles):\n",
    "  # TODO: write your code here\n",
    "\n",
    "run_pipeline(read_movie_names, \n",
    "             pcollection_fn=netflix_movie_title_collection, \n",
    "             data_file=str(DATASET_FOLDER/\"movie_titles.csv\"))\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!head -10 $OUTPUT_FOLDER/\"read_movie_names.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Hint\n",
    "\n",
    "#@markdown Use the `netflix_movie_titles_collection` function from the `movie_titles.csv` data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pFwELflRJNjy"
   },
   "outputs": [],
   "source": [
    "#@title Solution\n",
    "def read_movie_names(movie_titles):\n",
    "    return (\n",
    "        movie_titles\n",
    "        | \"Extract movie names\">> beam.Map(lambda title: title.name))\n",
    "\n",
    "run_pipeline(read_movie_names, \n",
    "             pcollection_fn=netflix_movie_title_collection, \n",
    "             data_file=str(DATASET_FOLDER/\"movie_titles.csv\"))\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!head -10 $OUTPUT_FOLDER/\"read_movie_names.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMhqdn4qN9bK"
   },
   "source": [
    "### Exercise 7b\n",
    "\n",
    "This part of the exercise is for learning of using `join` (which is called `CoGroupByKey` in **Apache Beam**).\n",
    "\n",
    "Output `PCollection((movie_name, number of views))`, only for movies with views.\n",
    "\n",
    "Hint: reuse the function for number of views per movie from Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mm43i16yPKfy"
   },
   "outputs": [],
   "source": [
    "def movie_name_count_views(movie):\n",
    "  # TODO: 1. reading movie titles similar to 7a\n",
    "\n",
    "  # TODO: 2. compute movie_id_counts\n",
    "\n",
    "  # TODO: 3. join 1 and 2 (search for CoGroupByKey)\n",
    "\n",
    "run_pipeline(movie_name_count_views)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!head -10 $OUTPUT_FOLDER/\"movie_name_count_views.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "VNzLCwawObWh"
   },
   "outputs": [],
   "source": [
    "#@title Solution\n",
    "def movie_name_count_views(views):\n",
    "    movie_views_count = views_count_per_movie(views)  # PCollection[[Id, Count]]\n",
    "    \n",
    "    pipeline = views.pipeline  # re-using the same pipeline\n",
    "    movie_title_ids = ( \n",
    "        netflix_movie_title_collection(pipeline) \n",
    "        | \"To (movie_id, title)\" >> beam.Map(lambda t: (t.movie_id, t))\n",
    "    )\n",
    "\n",
    "    # Join movie_views_count and movie_title_ids\n",
    "    def process_merge(merge_dict):\n",
    "        # Element Format: (movie_id, {\"counts\": [...], \"titles\": [...]})\n",
    "        movie_id, join_info = merge_dict\n",
    "        \n",
    "        if not join_info:\n",
    "            return # Skip\n",
    "        \n",
    "        if not len(join_info[\"counts\"]) or not len(join_info[\"titles\"]): \n",
    "            return [] # no views of this movie. It happens because only part of the views dataset is processed.\n",
    "        \n",
    "        yield (join_info[\"titles\"][0].name, join_info[\"counts\"][0])\n",
    "\n",
    "    return (\n",
    "        ({\"counts\": movie_views_count, \"titles\": movie_title_ids}) \n",
    "        | \"Merge\" >> beam.CoGroupByKey()\n",
    "        | beam.ParDo(process_merge))\n",
    "\n",
    "run_pipeline(movie_name_count_views)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!head -10 $OUTPUT_FOLDER/\"movie_name_count_views.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTauFvnvlF1c"
   },
   "source": [
    "## What's Next\n",
    "\n",
    "The next [colab](https://github.com/OpenMined/PipelineDP/blob/main/docs/tutorial_1/2_beam_laplace_mechansim.ipynb) introduces Laplace mechanism and how to use it."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1_beam_introduction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
