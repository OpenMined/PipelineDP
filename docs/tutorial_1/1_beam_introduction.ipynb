{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F92JfP83EUrU"
   },
   "source": [
    "# Introduction to Apache Beam API\n",
    "\n",
    "This notebook contains interactive exercises which introduces to [**Apache Beam**](https://beam.apache.org/) API which are needed for doing Differentially Private (`DP`) computations. \n",
    "\n",
    "In this notebook, we will first familiarise with Apache Beam and its [Python SDK](https://beam.apache.org/documentation/sdks/python/) by walking through multiple exercises of increasing complexity. \n",
    "\n",
    "**Don't worry**: if you are not yet familiar with Apache Beam and its Python APIs, each exercise will include a _Hints_ subsection that will point you in the right direction, suggesting what you should be looking at in the documentation to solve the exercise. Then, if you are still struggling with the exercise - or simply to compare your results - you can look at the attached _Solution_.\n",
    "\n",
    "In the next notebook, we will use Apache Beam to implement a fully _differentially private_ data analysis pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "We recommend opening and running this notebook in **Google Colab** by simply clicking on the badge below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PipelineDP/blob/main/docs/tutorial_1/1_beam_introduction.ipynb)\n",
    "\n",
    "[![nbviewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/OpenMined/PipelineDP/blob/main/docs/tutorial_1/1_beam_introduction.ipynb)\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/view%20on-GitHub-blue)](https://github.com/OpenMined/PipelineDP/blob/main/docs/tutorial_1/1_beam_introduction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once in Colab, here is what you should do: \n",
    "\n",
    "1. Click \"Run in Google Colab\" link above\n",
    "2. Make a copy in Drive (File $\\mapsto$ Save a copy in Drive)\n",
    "3. Run all the cells in **Setup** area\n",
    "4. Enjoy the exercises :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "XwvmAL_abu6w",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apache_beam package available and imported correctly ✅\n",
      "Apache Beam Python SDK version: 2.28.0\n"
     ]
    }
   ],
   "source": [
    "#@title Setup: Install and import `apache-beam` \n",
    "from IPython.display import clear_output\n",
    "try:\n",
    "    import apache_beam as beam\n",
    "except ImportError:\n",
    "    !pip install \"apache-beam[interactive]\"    \n",
    "    clear_output()\n",
    "finally:\n",
    "    import apache_beam as beam\n",
    "    print(\"apache_beam package available and imported correctly ✅\")\n",
    "    print(f\"Apache Beam Python SDK version: {beam.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "In this notebook we will be using the [**Netflix Prize Dataset**](https://en.wikipedia.org/wiki/Netflix_Prize) as a driver example.\n",
    "\n",
    "> The Netflix Prize was an open competition for the best collaborative filtering algorithm to predict user ratings for films, based on previous ratings without any other information about the users or films, i.e. without the users or the films being identified except by numbers assigned for the contest.\n",
    "\n",
    "(_Source_: Wikipedia)\n",
    "\n",
    "The choice of this dataset is particularly interesting from our perspective of _differentially private_ data pipelines, since this dataset was used as the reference example to present a new class of statistical de-anonymisation attacks against high-dimensional data. \n",
    "More details on these techniques, and their application to the `Netflix Prize` dataset are available in this [article](https://www.schneier.com/blog/archives/2007/12/anonymity_and_t_2.html), and in the original paper $\\Rightarrow$ [**Robust De-anonymization of Large Sparse Datasets**](https://www.cs.utexas.edu/~shmat/shmat_oak08netflix.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Netflix-Prize` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Netflix Prize dataset (`Netflix-prize`) is available on [Kaggle](https://www.kaggle.com/netflix-inc/netflix-prize-data), so to download the dataset we need to configure Python [`kaggle`](https://pypi.org/project/kaggle/) official API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(A)** Let's first setup the `DATASET_FOLDER` where the dataset will be downloaded and saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run to set up Data and Output folders { display-mode: \"form\" }\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "NETFLIX_PRIZE_FOLDER = Path(\"netflix_prize_dataset\")\n",
    "DATASET_FOLDER = NETFLIX_PRIZE_FOLDER / \"data\"\n",
    "OUTPUT_FOLDER = NETFLIX_PRIZE_FOLDER  / \"outputs\"\n",
    "\n",
    "DATA_FILE = str(DATASET_FOLDER / \"movie_views.txt\")\n",
    "OUTFILE_TEMPLATE = str(OUTPUT_FOLDER / '{}.txt')\n",
    "\n",
    "os.makedirs(DATASET_FOLDER, exist_ok=True)\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(B)** Download the `Netflix-prize` dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle package imported correctly ✅\n"
     ]
    }
   ],
   "source": [
    "#@title Run to setup `kaggle` Python API { display-mode: \"form\" }\n",
    "try:\n",
    "    import kaggle\n",
    "except ImportError:\n",
    "    !pip install kaggle\n",
    "    clear_output()\n",
    "except OSError:\n",
    "    print(\"Kaggle package is available, but credentials needs to be setup.\")\n",
    "else:\n",
    "    print(\"kaggle package imported correctly ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create and place your API token**\n",
    "\n",
    "In order to download datasets using the Kaggle API, please create and download your API token from Kaggle website:\n",
    "\n",
    "1. Log in to [Kaggle](https://www.kaggle.com/) website.\n",
    "2. Go to your **Account** (locate your profile avatar in the top-right corner of the page)\n",
    "\n",
    "![kaggle-account](images/kaggle_account.png)\n",
    "\n",
    "3. Scroll down your account page until you find the **API cell**.\n",
    "4. Click on to \"Create new API token\" button.\n",
    "\n",
    "![kaggle-api-key](images/api_key.png)\n",
    "\n",
    "This will download a file called `kaggle.json`. This file contains the credentials required to use the Kaggle APIs. \n",
    "\n",
    "5. Open this file, and include this data into the form below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle API credentials found ✅ - All set, Ready to start!\n"
     ]
    }
   ],
   "source": [
    "#@title Setup Kaggle API credentials { display-mode: \"form\" }\n",
    "from ipywidgets import Password\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "import os\n",
    "import stat\n",
    "import json\n",
    "\n",
    "kaggle_api_credentials = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "\n",
    "API_CREDENTIALS_EXIST = os.path.exists(kaggle_api_credentials) \n",
    "\n",
    "if not API_CREDENTIALS_EXIST:\n",
    "    user = widgets.Text(\n",
    "        placeholder='Insert your Kaggle User Name!',\n",
    "        description='User Name',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "    pwd = Password(\n",
    "        description='Key',\n",
    "        placeholder='Insert key for user {}'.format(user.value)\n",
    "    )\n",
    "\n",
    "    def store_kaggle_credentials(wdgt):\n",
    "        folder = os.path.expanduser('~/.kaggle')\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        file = '{}/kaggle.json'.format(folder)\n",
    "        with open(file, 'w') as kaggle_file:\n",
    "            json.dump({\n",
    "                'username': user.value,\n",
    "                'key': wdgt.value\n",
    "            }, kaggle_file)\n",
    "\n",
    "            print(\"Credentials created in {} ✅\".format(folder))\n",
    "        os.chmod(file, stat.S_IREAD | stat.S_IWRITE)\n",
    "\n",
    "    user.on_submit(lambda wdg: display(pwd))\n",
    "    pwd.on_submit(store_kaggle_credentials)\n",
    "\n",
    "    display(user)\n",
    "else:\n",
    "    print(\"Kaggle API credentials found ✅ - All set, Ready to start!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Authenticate on Kaggle via APIs { display-mode: \"form\" }\n",
    "import kaggle\n",
    "\n",
    "kaggle.api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref                                                    title                                             size  lastUpdated          downloadCount  voteCount  usabilityRating  \r\n",
      "-----------------------------------------------------  -----------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \r\n",
      "netflix-inc/netflix-prize-data                         Netflix Prize data                               683MB  2019-11-13 18:39:37          45080        878  0.7647059        \r\n",
      "stephanerappeneau/350-000-movies-from-themoviedborg    350 000+ movies from themoviedb.org               67MB  2017-10-12 19:49:17           4592        147  0.7647059        \r\n",
      "gspmoreira/articles-sharing-reading-from-cit-deskdrop  Articles sharing and reading from CI&T DeskDrop    8MB  2017-08-27 21:33:01           9319        127  0.8235294        \r\n",
      "arashnic/book-recommendation-dataset                   Book Recommendation Dataset                       24MB  2020-11-29 01:29:41            556         22  1.0              \r\n",
      "arashnic/marketing-bias-dataset                        Amazon Products Sold on ModCloth                   1MB  2020-12-15 23:51:21             36          5  0.9705882        \r\n",
      "clementmsika/mubi-sqlite-database-for-movie-lovers     🎬 MUBI SVOD Platform Database for Movie Lovers     1GB  2020-06-21 17:55:36            505         11  1.0              \r\n",
      "davidgrabois/data-for-netflix-prize-compatition        data_for_netflix_prize_compatition                 1MB  2020-11-10 17:10:15              4          0  0.125            \r\n",
      "lauraschiatti/netflix-prize                            netflix_prize                                    622MB  2020-07-14 09:18:16             80          0  0.11764706       \r\n",
      "arnavraj01/movies-rating                               Movies Rating                                    171MB  2021-01-31 04:52:50              6          0  0.4117647        \r\n"
     ]
    }
   ],
   "source": [
    "#@title **Search** on Kaggle for available Netflix-Prize datasets { display-mode: \"form\" }\n",
    "!kaggle datasets list -s \"netflix-prize\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top on the list is the dataset we are looking for: `netflix-inc/netflix-prize-data`.\n",
    "\n",
    "The dimension of the archive is `638 MB`, which means it may take some time to download and uncompress. (⚠️ **this may take sometime, depending on your internet connection**) Please keep this in mind while you will run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1.00M/683M [00:00<01:24, 8.46MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading netflix-prize-data.zip to netflix_prize_dataset/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 683M/683M [01:24<00:00, 8.50MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Run this cell to initiate the download { display-mode: \"form\" }\n",
    "if not len(os.listdir(DATASET_FOLDER)) or not os.path.exists(DATA_FILE):\n",
    "    # Checking here to avoid download multiple times in case of notebook re-run\n",
    "    kaggle.api.dataset_download_files(\"netflix-inc/netflix-prize-data\", \n",
    "                                      path=DATASET_FOLDER, unzip=True, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie_titles.csv',\n",
       " 'qualifying.txt',\n",
       " 'combined_data_2.txt',\n",
       " 'combined_data_3.txt',\n",
       " 'combined_data_1.txt',\n",
       " 'combined_data_4.txt',\n",
       " 'README',\n",
       " 'probe.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title **Check** that everything was downloaded ok { display-mode: \"form\" }\n",
    "os.listdir(DATASET_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Netflix-prize` data files\n",
    "\n",
    "Before getting into the actual processing of this dataset with **Apache Beam**, let's briefly describe the actual data files included in the Kaggle archive. \n",
    "\n",
    "The dataset is organised into multiple files, accounting for over `100` million ratings from `480` thousand\n",
    "randomly-chosen, anonymous Netflix customers, over `17` thousand movie titles. \n",
    "\n",
    "- The `probe.txt` and the `qualitfying.txt` files are needed for the submission on Kaggle, so discarded as not interesting here.\n",
    "\n",
    "- Movie information (i.e. _metadata_) are contained in the `movie_titles.txt` in the following format:\n",
    "\n",
    "```\n",
    "MovieID,YearOfRelease,Title\n",
    "\n",
    "- MovieID is a general (randomised) ID for movies, rangin from 1 to 17,770.\n",
    "- YearOfRelease can range from 1,890 to 2,005.\n",
    "- Title (in English) is the Netflix title.\n",
    "```\n",
    "\n",
    "- The various `combined_data_*.txt` contain the actual data in the following format:\n",
    "\n",
    "```\n",
    "MovieID:\n",
    "CustomerID,Rating,Date\n",
    "CustomerID,Rating,Date\n",
    "....\n",
    "\n",
    "- MovieIDs matches those in the list of \"movie_titles.txt\".\n",
    "- CustomerIDs range from 1 to 2,649,429, with gaps. There are 480,189 users.\n",
    "- Ratings are on a five star (integral) scale from 1 to 5.\n",
    "- Dates have the format YYYY-MM-DD.\n",
    "```\n",
    "\n",
    "A more thorough description of each of those files is available in the [`README`](netflix_prize_dataset/data/README) file. \n",
    "\n",
    "\n",
    "The total dimension of the whole dataset is around `2GB` - considering all the `combined_data_*.txt` data files together - and computing times are prohibitive when executed interactively in Colab. \n",
    "\n",
    "Therefore, without any loss of generality, we will use only `combined_data_1.txt` as for the reference data file (i.e. `movie_views.txt`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing our dataset \n",
    "\n",
    "The size of the full and complete dataset is around `2GB`, if combining together all the `combined_data_` files. This means that we won't be able to process the entire collection of data as quickly and interactive as we would hope while running a notebook on our laptop - despite all the boosting with `beam` we will be setting up, nonetheless. \n",
    "\n",
    "Therefore we will limit our analysis to a reduced version of the dataset. \n",
    "\n",
    "However, you'll get to choose how big this dataset will be, depending on the resources you have on your computer, and the amount of time you'd like to wait for the computation to finish :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_data_1.txt:  24,058,263\n",
      "combined_data_2.txt:  26,982,302\n",
      "combined_data_3.txt:  22,605,786\n",
      "combined_data_4.txt:  26,851,926\n",
      "Total Lines Count  : 100,498,277\n"
     ]
    }
   ],
   "source": [
    "#@title Let's see how many lines each data file contains { display-mode: \"form\" }\n",
    "data_files = filter(lambda f: f.startswith(\"combined_data_\"), os.listdir(DATASET_FOLDER))\n",
    "lines_count = 0\n",
    "for data_file in sorted(data_files):\n",
    "    file_path = DATASET_FOLDER / data_file\n",
    "    wc_l = !wc -l $file_path\n",
    "    lines, _ = wc_l[0].strip().split()\n",
    "    lines_count += int(lines)\n",
    "    print(f\"{data_file}:  {int(lines):,}\")\n",
    "print(f\"Total Lines Count  : {lines_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these statistics, we can immediately notice that each data file roughly contains the same number of lines, which means that the amout of time required to process each one of them will be more or less the same.\n",
    "\n",
    "However, even just limiting our analysis to one single file (i.e. `>20M` lines) might be prohibitive, and indeed beyond of the scope of this tutorial. The **main goal** of the tutorial is to demostrate the ability of `apache_beam` to scale up the computation on large dataset. For real scalability, we also need decent computation capacity. \n",
    "\n",
    "Therefore, let's decide how many lines we want our _reduced_ dataset to be (**default**: `1M` lines). \n",
    "\n",
    "➡️ If you are up to process a much larger dataset, please feel free to choose a different number of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Number of Lines: 1,000,000\n"
     ]
    }
   ],
   "source": [
    "#@title Choose the size (in lines) of the Netflix-prize Dataset used {display-mode: \"form\"}\n",
    "\n",
    "# Note: this piece of code is a bit weird when read into a notebook, but it makes much\n",
    "# more sense if run into Google Colab as it uses the new Form feature\n",
    "DATASET_SIZE = \"1M lines\" #@param [\"10K lines\", \"100K lines\", \"500K lines\", \"1M lines\", \"5M lines\", \"10M lines\", \"15M lines\", \"20M lines\", \"Full Dataset\"] {type: \"string\"}\n",
    "\n",
    "if DATASET_SIZE == \"Full Dataset\":\n",
    "    DS_LINES = 24058263  # whole combined_data_1.txt file\n",
    "else:\n",
    "    DS_LINES = int(DATASET_SIZE.replace(\" lines\", \"\").replace(\"K\", \"000\").replace(\"M\", \"000000\"))\n",
    "\n",
    "print(\"Selected Number of Lines: {:,}\".format(DS_LINES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Create** the reference data file `movie_views.txt` { display-mode: \"form\" }\n",
    "\n",
    "!head -n $DS_LINES $DATASET_FOLDER/combined_data_1.txt > $DATA_FILE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20M\tnetflix_prize_dataset/data/movie_views.txt\r\n"
     ]
    }
   ],
   "source": [
    "!du -h $DATA_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Well done! \n",
    "\n",
    "The setup of the `Netflix-prize` dataset is now complete! Now it is time to start setting up our execution framework based on **Apache Beam**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Execution Framework\n",
    "\n",
    "In this section we will define the core main components that will be used throughout the exercises. These components will be based on **Apache Beam**, which consitutes the reference computational framework, and provides the building blocks to define our data workflows.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apache_beam` in a Nutshell\n",
    "\n",
    "Apache Beam adheres to a programming model which is centred around a few key elements:\n",
    "\n",
    "* `Pipeline`: determines the **main** _data processing workflow_ as defined as a _sequence_ of **operations** on the data. This sequence is internally encoded as a directed acyclic graph in which nodes and arcs correspond to instances of `PCollection` and `PTransform`, respectively. \n",
    "\n",
    "\n",
    "- `PCollection`: represents a generic (**parallel**, `P`) data collection that can be either _bounded_ or _unbounded_ in size.\n",
    "\n",
    "\n",
    "- `PTransform`: encodes a single (transform) operation. Each single `PTransform` accepts a `PCollection` in input, and it is expected to return a new `PCollection`. Beam provides some [**core**](https://beam.apache.org/documentation/programming-guide/#core-beam-transforms) transformers (e.g. [`ParDo`](https://beam.apache.org/documentation/programming-guide/#pardo), [`Flatten`](https://beam.apache.org/documentation/programming-guide/#flatten), and [`Partition`](https://beam.apache.org/documentation/programming-guide/#partition)) which correspondo to specific processing paradigms.\n",
    "\n",
    "\n",
    "- `Runner`: (or `PipelineRunner`) determines the computational environment in which the whole data processing will be executed. \n",
    "\n",
    "Finally, pipelines I/O operations are supported by the `apache_beam` framework as core primitives (i.e. `PTransform`) to `Read` and `Write` from and to external sources.\n",
    "\n",
    "$\\Rightarrow$ More information on the Basics of [Beam Model](https://beam.apache.org/documentation/basics/) and on [Pipeline I/O](https://beam.apache.org/documentation/programming-guide/#pipeline-io) are available in the online documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Execution Modules on `Netflix-Prize` dataset\n",
    "\n",
    "Despite of its simplicity, the reference programming model in Apache Beam accounts for one very important feature: **composability**. In fact, it is possible to either re-use (part of) a `Pipeline` within other and more complex workflows, or simply chaining multiple pipelines together. \n",
    "\n",
    "With specific reference to the `Netflix-prize` dataset, the general default workflow is composed by the following steps:\n",
    "\n",
    "1. **[S1]** Read from the `DATA_FILE`, and generate the reference `PCollection`\n",
    "    - `S1.1` A proper **data abstraction** is necessary to encapsulate the information for each _movie view_;\n",
    "    - `S1.2` Data I/O will be performed in a parallel fashion (i.e. `ParDo`), leveraging on one of the most important feature of Apache Beam: _Data Parallelization_. \n",
    "\n",
    "\n",
    "2. **[S2]** Pass on the generated `PCollection` to the core (_abstract_) processing module \n",
    "    - `S2.1` Each core module will be expected as a _callable_ (implemented in the following exercises) to return the final `PCollection` to store and save (see `S3`).\n",
    "\n",
    "3. **[S3]** Write the results to the corresponding `OUTPUT_FILE`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`S1.1`** Data Abstraction\n",
    "\n",
    "To encapsulate the information for each _movie view_, we will make use of Python 3.7+ `dataclass` (see [doc](https://docs.python.org/3.8/library/dataclasses.html)). \n",
    "\n",
    "Our reference `MovieView` dataclass model is very simple, and will represent the atomic **data object** included in the `netflix-prize` `PCollection`. \n",
    "\n",
    "To simplify objects instantiation, the `date_stamp` init attribute (i.e. [`InitVar`](https://docs.python.org/3.8/library/dataclasses.html#init-only-variables)), will be later converted with proper data type and format in the `date` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, InitVar\n",
    "from datetime import datetime\n",
    "\n",
    "@dataclass\n",
    "class MovieView:\n",
    "    user_id: int\n",
    "    movie_id: int\n",
    "    rating: int\n",
    "    date_stamp: InitVar[str]\n",
    "    date: datetime.date = None\n",
    "    \n",
    "    def __post_init__(self, date_stamp):\n",
    "        if self.date is None and date_stamp is not None:\n",
    "            self.date = datetime.strptime(date_stamp, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Python Type Safety and Custom objects\n",
    "\n",
    "The Apache Beam SDK for Python uses type hints during pipeline construction and runtime to try to emulate the correctness guarantees achieved by true static typing. Additionally, using type hints lays some groundwork that allows the backend service to perform efficient type deduction and registration of `Coder` objects.\n",
    "\n",
    "Since in this tutorial we want to handle our own custom **data object**, we also need to implement our custom (_yet simple_) `beam.coders.Coder` to allow for a correct object _serialisation_.\n",
    "\n",
    "**Note**: This step is particularly crucial during the execution (_in parallel_) of the Pipeline, to avoid `apache_beam` to fallback to the default `Serializer` (i.e. `Coder`) based in `pickle`.\n",
    "More information on **Python Type Safety** in Apache Beam pipelines is available in the official [documentation](https://beam.apache.org/documentation/sdks/python-type-safety/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieViewCoder(beam.coders.Coder):\n",
    "    \"\"\"Beam Coder Serialiser for our custom MovieView Data Object\"\"\"\n",
    "    def encode(self, view):\n",
    "        return (f\"{view.user_id}:{view.movie_id}:{view.rating}:{str(view.date.date())}\").encode(\"utf-8\")\n",
    "\n",
    "    def decode(self, s):\n",
    "        return MovieView(*s.decode(\"utf-8\").split(':'))\n",
    "\n",
    "    def is_deterministic(self):\n",
    "        return True\n",
    "\n",
    "# Once defined, we can register the new Coder\n",
    "beam.coders.registry.register_coder(MovieView, MovieViewCoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Workaround `multiprocessing` issues with namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **Note**: As a workaround to enable _parallel computation_ in the notebook via Python `multiprocessing` and `beam.DoFn` (see next section), the dataclass and its coder will be saved into an external module (i.e. `movie_view.py`) for later use via **direct import**. This will guarantee a proper object serialisation, while also adhering to **Apache Beam** requirements. See [Requirements for writing user code for beam transforms](https://beam.apache.org/documentation/programming-guide/#requirements-for-writing-user-code-for-beam-transforms) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "echo -e \"\n",
    "from dataclasses import dataclass, InitVar\n",
    "from datetime import datetime\n",
    "import apache_beam as beam\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MovieView:\n",
    "    user_id: int\n",
    "    movie_id: int\n",
    "    rating: int\n",
    "    date_stamp: InitVar[str]\n",
    "    date: datetime.date = None\n",
    "    \n",
    "    def __post_init__(self, date_stamp):\n",
    "        if self.date is None and date_stamp is not None:\n",
    "            self.date = datetime.strptime(date_stamp, '%Y-%m-%d')\n",
    "            \n",
    "\n",
    "class MovieViewCoder(beam.coders.Coder):\n",
    "    \\\"\\\"\\\"Beam Coder Serialiser for our custom MovieView Data Object\\\"\\\"\\\"\n",
    "    def encode(self, view):\n",
    "        return (f'{view.user_id}:{view.movie_id}:{view.rating}:{str(view.date.date())}').encode('utf-8')\n",
    "\n",
    "    def decode(self, s):\n",
    "        return MovieView(*s.decode('utf-8').split(':'))\n",
    "\n",
    "    def is_deterministic(self):\n",
    "        return True\n",
    "\n",
    "# Once defined, we can register the new Coder\n",
    "beam.coders.registry.register_coder(MovieView, MovieViewCoder)\n",
    "\" > movie_view.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MovieView dataclass has been successfully imported.\n"
     ]
    }
   ],
   "source": [
    "#@title **Verify** that import from movie_view module works properly { display-mode: \"form\" } \n",
    "try: \n",
    "    from movie_view import MovieView\n",
    "except ImportError:\n",
    "    print(\"movie_view module cannot be imported. Please check!\")\n",
    "else:\n",
    "    print(\"MovieView dataclass has been successfully imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`S1.2`** `beam.ParDo` handler\n",
    "\n",
    "The [`beam.ParDo`](https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.core.html#apache_beam.transforms.core.ParDo) processing paradigm is very similar to the `Map` phase of a `Map`/`Shuffle`/`Reduce`-style algorithm. \n",
    "\n",
    "The [`ParDo`](https://beam.apache.org/documentation/transforms/python/elementwise/pardo/) transformer is executed in parallel elementwise to the `PCollection`, via custom [`beam.DoFn`](https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.core.html#apache_beam.transforms.core.DoFn) callable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "@beam.typehints.with_output_types(MovieView)  # type-hint annotation for the output\n",
    "class ParseMovieViews(beam.DoFn):\n",
    "    \"\"\"ParDo main execution core. Each line of the DATA_FILE will be processed, \n",
    "    and an instance of MovieView will be created and returned, accordingly.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.movie_id: int = -1\n",
    "    \n",
    "    def process(self, line: str):\n",
    "        from movie_view import MovieView\n",
    "        line = line.strip()  # get rid of any useless tabulation\n",
    "        if not line:\n",
    "            return  # Skip Line\n",
    "        if line.endswith(\":\"):\n",
    "            movie_id, _ = line.split(\":\")\n",
    "            self.movie_id = int(movie_id)\n",
    "            return  # Automatically skip this line\n",
    "        user_id, rating, date_stamp, *_ = line.split(',')\n",
    "        user_id, rating = int(user_id), int(rating)\n",
    "        yield MovieView(user_id, self.movie_id, rating, date_stamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ParseMovieViews` callable will be used to generate the initial reference `PCollection` for the `Netflix-prize` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netflix_movie_views_collection(p: beam.Pipeline, data_file: str = DATA_FILE) -> beam.PCollection:\n",
    "    \"\"\"Generate the initial (Parallel) Collection of \n",
    "    `MovieView` objects as extracted from the input `data_file`\"\"\"\n",
    "    return (\n",
    "        p\n",
    "        | beam.io.ReadFromText(data_file)\n",
    "        | beam.ParDo(ParseMovieViews()).with_output_types(MovieView)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`S2.1`** The core `beam.Pipeline`\n",
    "\n",
    "Now it is finally the time to create the whole `beam.Pipeline` infrastructure, that will allow pluggable callables to be used (as implemented in the following Exercises).\n",
    "The Pipeline will be also configured to leverage as much as possible parallel execution in a way that we could circumvent the Python [**GIL**](https://wiki.python.org/moin/GlobalInterpreterLock)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Multiprocessing Start method:  spawn\n",
      "Multiprocessing available Cores:  8\n"
     ]
    }
   ],
   "source": [
    "#@title **Check** Python multiprocessing settings { display-mode: \"form\" }\n",
    "import multiprocessing as mp\n",
    "\n",
    "print(\"Python Multiprocessing Start method: \", mp.get_start_method())\n",
    "print(\"Multiprocessing available Cores: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define `PipelineModule` type for accepted callables**\n",
    "\n",
    "(The type hints is mainly for design and documentation purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# A PipelineModule is defined as a Callable that accepts any parameter\n",
    "# and returns a beam.PCollection` instance\n",
    "PipelineModule = Callable[[beam.PCollection], beam.PCollection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from apache_beam.runners import DirectRunner\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "def run_pipeline(pipeline_fn: PipelineModule, mode: str = \"multi_processing\", \n",
    "                 data_file: str = DATA_FILE):\n",
    "    start_time = time.time()\n",
    "    fn_name = pipeline_fn.__name__\n",
    "    outfile = str(OUTFILE_TEMPLATE).format(fn_name)  # output logfile\n",
    "\n",
    "    # Pipeline Options\n",
    "    pipeline_options = PipelineOptions.from_dictionary(\n",
    "        {\n",
    "            \"direct_num_workers\": 0,  # setting parallelism to CPU count\n",
    "            \"direct_running_mode\": mode,  # work around GIL\n",
    "            \"job-server-timeout\": 1048576,  # used to avoid potential timeout interruptions of workers\n",
    "        }\n",
    "    )\n",
    "    with beam.Pipeline(runner=DirectRunner(), options=pipeline_options) as p:\n",
    "        views_coll = netflix_movie_views_collection(p, data_file=data_file)\n",
    "        pipeline_fn(views_coll) | \"Write result\" >> beam.io.WriteToText(\n",
    "            outfile, shard_name_template=\"\"\n",
    "        )\n",
    "    # Timing\n",
    "    elapsed_time = time.time() - start_time\n",
    "    hours, remainder = divmod(elapsed_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f\"Elapsed time in {fn_name} is {hours:2.0f}H {minutes:2.0f}m {seconds:2.0f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.transforms import combiners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428688,3,2005-01-31\r\n",
      "2225578,2,2005-01-31\r\n",
      "1656194,2,2005-02-01\r\n",
      "1768021,3,2005-02-08\r\n",
      "747879,2,2005-02-08\r\n",
      "1196927,3,2005-02-15\r\n",
      "528854,5,2005-02-21\r\n",
      "962705,3,2005-02-22\r\n",
      "1299323,2,2005-02-24\r\n",
      "2026970,4,2005-02-27\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n 10 $DATA_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of§a\n",
    "# The exercise framework calls this funciton with |views| representing all MovieViews in the dataset.\n",
    "# |views| has type PCollection, which is a parrallel collection.\n",
    "# Beam pipelines consist of applying PTransforms (i.e. parallel transformations) on PCollections\n",
    "def count_all_views(views: beam.PCollection): \n",
    "   # This is an example of applying PTransform\n",
    "   # Beam uses | (pipe) syntax:\n",
    "   # new_p_collection = p_collection | \"Optional comment\" >> PTransform\n",
    "   return views | \"Count all views\" >> combiners.Count.Globally().with_input_types(MovieView)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10000000 $DATASET_FOLDER/combined_data_1.txt > $DATASET_FOLDER/tenmillionlines.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201M\tnetflix_prize_dataset/data/tenmillionlines.txt\r\n"
     ]
    }
   ],
   "source": [
    "!du -h $DATASET_FOLDER/tenmillionlines.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "IsZ2pz__Ajiz",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time in count_all_views is  0H  1m 22s\n"
     ]
    }
   ],
   "source": [
    "run_pipeline(count_all_views, mode=\"multi_processing\", data_file=str(DATASET_FOLDER/\"tenmillionlines.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "IsZ2pz__Ajiz",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time in count_all_views is  0H  5m 57s\n"
     ]
    }
   ],
   "source": [
    "run_pipeline(count_all_views, mode=\"in_memory\", data_file=str(DATASET_FOLDER/\"tenmillionlines.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "IsZ2pz__Ajiz",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time in count_all_views is  0H  6m 15s\n"
     ]
    }
   ],
   "source": [
    "run_pipeline(count_all_views, mode=\"multi_threading\", data_file=str(DATASET_FOLDER/\"tenmillionlines.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First lines of the output file:\n",
      "9998038\r\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst lines of the output file:\")\n",
    "!head -1 $OUTPUT_FOLDER/\"count_all_views.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuPWswzQyBF0"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "Each exercise it to write an Apache Beam pipeline, which does some transformations on a part of Netflix prize dataset data (the first 10K lines). \n",
    "\n",
    "The completion of the exercises requires **additional research** on usage of Beam API with a search engine or looking at the offcial [API reference](https://beam.apache.org/documentation/sdks/python/). \n",
    "\n",
    "Let's start by looking at first lines of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0878_11KzO2z",
    "outputId": "f43f5e0a-e620-4c36-ae85-eb0cec5cec13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:\r\n",
      "1488844,3,2005-09-06\r\n",
      "822109,5,2005-05-13\r\n",
      "885013,4,2005-10-19\r\n",
      "30878,4,2005-12-26\r\n"
     ]
    }
   ],
   "source": [
    "# There are 2 types of lines '<movie_id>:' and '<user_id>, <rating>, <date>'. \n",
    "# '<movie_id>:' says that the following views have movie ids <movie_id>\n",
    "# Rating is in 1..5\n",
    "!head -5 $DATA_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.transforms import combiners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spawn'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.get_start_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFirst lines of the output file:\")\n",
    "!head -1 $OUTPUT_FOLDER/\"count_all_views.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BPomLO8zy4t"
   },
   "source": [
    "The exercise framework function **run_pipeline** executes a user defined pipeline function.\n",
    "The output of the pipeline is written in a file 'content/outputs/(pipeline_fn name)'.\n",
    "\n",
    "Each exercise has a solution. \n",
    "\n",
    "The exercises don't assume prior knowledge of Apache Beam. This Colab shows basically all Beam API which is needed for working on Privacy Beam. It assumes that neccassary API is searched on the Internet.\n",
    "\n",
    "In all exercises elements of the input collection are MovieView objects:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhPG9AAiFGns"
   },
   "source": [
    "Let's start with an example: the goal is to write a pipeline function for computing number of records in the dataset. The example code is in the following code cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJN_Icn2DDAi"
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "Write a pipeline function for computing distinct movies IDs in the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "9aSeuIrMD6-z"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-42-8486879f73cd>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-42-8486879f73cd>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    run_pipeline(distinct_movies_ids)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def distinct_movies_ids(views):\n",
    "    # TODO: write your code here\n",
    "    pass\n",
    "\n",
    "run_pipeline(distinct_movies_ids)\n",
    "\n",
    "print(\"\\Content of the output file:\")\n",
    "!cat $OUTPUT_FOLDER/\"distinct_movies_ids.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Sl4XO84sEJZy"
   },
   "outputs": [],
   "source": [
    "#@title Hint (double click to open)\n",
    "# Search for beam.Map and beam.Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "cellView": "form",
    "id": "XtyCCoHJDCUr",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time in distinct_movies_ids is  0H  0m  3s\n",
      "\n",
      "Content of the output file:\n",
      "8\r\n",
      "7\r\n",
      "5\r\n",
      "2\r\n",
      "4\r\n",
      "6\r\n",
      "1\r\n",
      "3\r\n"
     ]
    }
   ],
   "source": [
    "#@title Solution (click on the arrow button to run, double click to open)\n",
    "def distinct_movies_ids(views):\n",
    "  movie_ids = views | beam.Map(lambda mv: mv.movie_id)\n",
    "  return movie_ids | beam.Distinct()\n",
    "\n",
    "run_pipeline(distinct_movies_ids)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!cat $OUTPUT_FOLDER/\"distinct_movies_ids.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnAQT1kzEfmL"
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "Write a pipeline function for computing the **number** of distinct movies ids in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jqgHgbfE1CD"
   },
   "outputs": [],
   "source": [
    "def number_distinct_movies_ids(views):\n",
    "  # TODO: write your code here\n",
    "  # Hint: you can use the function from the previous exercise\n",
    "\n",
    "run_pipeline(number_distinct_movies_ids)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!cat \"outputs/number_distinct_movies_ids\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "KVJ1EqOMFaWj"
   },
   "outputs": [],
   "source": [
    "#@title Solution (click on the arrow button to run, double click to open)\n",
    "def number_distinct_movies_ids(views):\n",
    "  return distinct_movies_ids(views) | combiners.Count.Globally()\n",
    "\n",
    "run_pipeline(number_distinct_movies_ids)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!cat \"outputs/number_distinct_movies_ids\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEg0IdtM-USW"
   },
   "source": [
    "## Exercise 3\n",
    "\n",
    "Write a pipeline which computes the number of views per movie i.e. it outputsPCollection (movie_id, number of views)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5F4sUYId-tKk"
   },
   "outputs": [],
   "source": [
    "def views_count_per_movie(views):\n",
    "   # TODO: write your code\n",
    "\n",
    "run_pipeline(views_count_per_movie)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!cat \"outputs/views_count_per_movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "vjfFjkVm_s5_"
   },
   "outputs": [],
   "source": [
    "#@title Hint (double click to open)\n",
    "# Search for beam combiners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "f5dIxHKt-u-d"
   },
   "outputs": [],
   "source": [
    "#@title Solution\n",
    "def views_count_per_movie(views):\n",
    "  movie_ids = views | beam.Map(lambda mv: mv.movie_id)\n",
    "  return movie_ids | \"Count movies\" >> combiners.Count.PerElement()\n",
    "\n",
    "run_pipeline(views_count_per_movie)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!cat \"outputs/views_count_per_movie\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gizp5nvB_P_X"
   },
   "source": [
    "## Exercise 4\n",
    "\n",
    "Write a pipeline which computes the average rating per movie, i.e. it outputs PCollection (movie_id, average rating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZP12hCn_1Pu"
   },
   "outputs": [],
   "source": [
    "def mean_rating_per_movie(views):\n",
    "  # TODO: write your code here\n",
    "\n",
    "run_pipeline(mean_rating_per_movie)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!cat \"outputs/mean_rating_per_movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "iZUtNUwg_wfm"
   },
   "outputs": [],
   "source": [
    "#@title Solution\n",
    "\n",
    "def mean_rating_per_movie(views):\n",
    "  # 2 element tuples then tuples are treated as (key, value) in PCollections\n",
    "  movie_rating = views | beam.Map(lambda mv: (mv.movie_id, mv.rating))\n",
    "  return movie_rating | combiners.Mean.PerKey()\n",
    "\n",
    "run_pipeline(mean_rating_per_movie)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!cat \"outputs/mean_rating_per_movie\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Rahkq8mAoyk"
   },
   "source": [
    "## Exercise 5\n",
    "\n",
    "Write a pipeline which bounds number of views per user by |max_views|, it outputs PCollection(MovieView), such that for each user_id there are not more than |max_views| records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SU5CKbs9BT_3"
   },
   "outputs": [],
   "source": [
    "def bound_number_of_views_per_user(views, max_views=2):\n",
    "  # TODO: write your code here\n",
    "\n",
    "run_pipeline(bound_number_of_views_per_user)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!head -5 \"outputs/bound_number_of_views_per_user\"\n",
    "\n",
    "print(\"\\nNumber of lines (the correct number is 9930 for max_views=2):\")\n",
    "!wc -l \"outputs/bound_number_of_views_per_user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "g8E6o2PgCpOH"
   },
   "outputs": [],
   "source": [
    "#@title Hint\n",
    "# Search for combiners.Sample and beam.FlatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "A4HuEH_lA5y2"
   },
   "outputs": [],
   "source": [
    "#@title Solution\n",
    "def bound_number_of_views_per_user(views,\n",
    "                                   max_views=2):\n",
    "  user_view = views | beam.Map(lambda mv: (mv.user_id, mv))\n",
    "  # user_id -> list of MovieView\n",
    "  user_bounded_views = user_view | combiners.Sample.FixedSizePerKey(max_views)\n",
    "  bounded_views = user_bounded_views | \"Remove keys\" >> beam.Values()\n",
    "  return bounded_views | \"Unnest lists\" >> beam.FlatMap(lambda x: x)\n",
    "\n",
    "run_pipeline(bound_number_of_views_per_user)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!head -5 \"outputs/bound_number_of_views_per_user\"\n",
    "print(\"\\nNumber of lines:\")\n",
    "!wc -l \"outputs/bound_number_of_views_per_user\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgNEnrKeHZUo"
   },
   "source": [
    "## Exercise 6\n",
    "Write a pipeline which computes rating histogram per movie, i.e. \n",
    "it outputs PCollection((movie_id, histogram_tuple)), where histogram_tuple is \n",
    "the 5 elements tuple, which contains the number of views with corresponding rating.\n",
    "\n",
    "This exercise is for learning how to create custom combiners (look for beam.CombineFn).\n",
    "\n",
    "Note: this exercise is bigger than previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmaNHtd5G1Xg"
   },
   "outputs": [],
   "source": [
    "def movie_rating_histogram(views):\n",
    "   # TODO: write your code here\n",
    "\n",
    "run_pipeline(movie_rating_histogram)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!head -10 \"outputs/movie_rating_histogram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lEZNOSX3C-xx"
   },
   "outputs": [],
   "source": [
    "#@title Solution\n",
    "class SumTuplesCombiner(beam.CombineFn):\n",
    "  def __init__(self, ndim):\n",
    "    self.ndim = ndim\n",
    "\n",
    "  def create_accumulator(self):\n",
    "    return [0] * self.ndim\n",
    "\n",
    "  def add_input(self, accumulator, element):\n",
    "    for i, b in enumerate(element):\n",
    "      accumulator[i] += b\n",
    "    return accumulator\n",
    "\n",
    "  def merge_accumulators(self, accumulators):\n",
    "    res = self.create_accumulator()\n",
    "    for a in accumulators:\n",
    "      self.add_input(res, a)\n",
    "    return res\n",
    "\n",
    "  def extract_output(self, accumulator):\n",
    "    return accumulator\n",
    "\n",
    "def movie_rating_histogram(views):\n",
    "  def one_hot(rating):\n",
    "    return (rating == 1, rating == 2, rating == 3, rating == 4, rating == 5)\n",
    "\n",
    "  movie_rating_vector = views | beam.Map(lambda mv: (mv.movie_id, one_hot(mv.rating)))\n",
    "  return movie_rating_vector | beam.CombinePerKey(SumTuplesCombiner(ndim=5))\n",
    "\n",
    "run_pipeline(movie_rating_histogram)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!head -10 \"outputs/movie_rating_histogram\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTCuMUNMHcVw"
   },
   "source": [
    "##Exercise 7\n",
    "\n",
    "This exercise has 2 parts, the first one is about reading from files, the second is about using join.\n",
    "\n",
    "In this exercise we will work with MovieTitle. The next code cell contains data class definition and code for parsing MovieTitle from string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoBYRKkZIbLx"
   },
   "outputs": [],
   "source": [
    "# MovieTitle class and parsing of Movie titles\n",
    "\n",
    "@dataclass\n",
    "class MovieTitle:\n",
    "  movie_id: int\n",
    "  year: int\n",
    "  name: str\n",
    "\n",
    "def parse_movie_title(line):\n",
    "  if line.strip() == \"\": return []\n",
    "  spl = line.split(',')\n",
    "  movie_id = int(spl[0])\n",
    "  year = -1 if spl[1] == \"NULL\" else int(spl[1])\n",
    "  title = spl[2]\n",
    "  return MovieTitle(movie_id, year, title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJit3uvPKiKa"
   },
   "source": [
    "### Exercise 7a\n",
    "Read and parse a movie titles file and output movie names. It should output PCollection(str).\n",
    "\n",
    "There is MOVIE_TITLES_FILE const with the path for the file. \n",
    "\n",
    "Hint: use MovieTitle and parse_movie_title from the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aw1p74fuNrhy"
   },
   "outputs": [],
   "source": [
    "def read_movie_names(_):  # _ is acctually PCollection of views, because the framework supports only views\n",
    "  pipeline = _.pipeline  # a pipeline object is needed as \"input\" for reading operation \n",
    "   # TODO: write your code here\n",
    "\n",
    "run_pipeline(read_movie_names)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!head -10 \"outputs/read_movie_names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pFwELflRJNjy"
   },
   "outputs": [],
   "source": [
    "#@title Solution\n",
    "def read_movie_names(_):\n",
    "  pipeline = _.pipeline\n",
    "  return (pipeline \n",
    "            | \"Read movie titles\" >> beam.io.ReadFromText(MOVIE_TITLES_FILE)\n",
    "            | \"Parse \">> beam.Map(parse_movie_title) \n",
    "            | \"Extract name\">> beam.Map(lambda title: title.name))\n",
    "\n",
    "run_pipeline(read_movie_names)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!head -10 \"outputs/read_movie_names\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMhqdn4qN9bK"
   },
   "source": [
    "### Exercise 7b\n",
    "\n",
    "This part of the exercise is for learning of using join (which is called CoGroupByKey in Beam).\n",
    "\n",
    "Output PCollection((movie_name, number of views)), only for movies with views.\n",
    "\n",
    "Hint: reuse the function for number of views per movie from Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mm43i16yPKfy"
   },
   "outputs": [],
   "source": [
    "def movie_name_count_views(views):\n",
    "  # TODO: 1. reading movie titles similar to 7a\n",
    "\n",
    "  # TODO: 2. compute movie_id_counts\n",
    "\n",
    "  # TODO: 3. join 1 and 2 (search for CoGroupByKey)\n",
    "\n",
    "run_pipeline(movie_name_count_views)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!head -10 \"outputs/movie_name_count_views\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "VNzLCwawObWh"
   },
   "outputs": [],
   "source": [
    "#@title Solution\n",
    "def movie_name_count_views(views):\n",
    "  movie_id_count = views_count_per_movie(views)\n",
    "  pipeline = views.pipeline\n",
    "  movie_titles = (pipeline \n",
    "            | \"Read movie titles\" >> beam.io.ReadFromText(MOVIE_TITLES_FILE)\n",
    "            | \"Parse \">> beam.Map(parse_movie_title)) \n",
    "\n",
    "  movie_id_title = movie_titles | \"To (movie_id, title)\" >> beam.Map(lambda t: (t.movie_id, t))\n",
    "\n",
    "  # Join movie_id_count and movie_id_title\n",
    "  COUNT_KEY = 0\n",
    "  TITLES_KEY = 1\n",
    "  # Element Format: (movie_id, {COUNT_KEY: [], TITLES_KEY: []})\n",
    "  join_res = {COUNT_KEY: movie_id_count, TITLES_KEY: movie_id_title} | beam.CoGroupByKey()\n",
    "\n",
    "  def process_join(movie_id_dict):\n",
    "    if len(movie_id_dict[1][COUNT_KEY]) == 0: return [] # no views of this movie. It happens because only part of the views dataset is processed.\n",
    "    yield (movie_id_dict[1][TITLES_KEY][0].name, movie_id_dict[1][COUNT_KEY][0])\n",
    "\n",
    "  return join_res | beam.ParDo(process_join)\n",
    "\n",
    "run_pipeline(movie_name_count_views)\n",
    "\n",
    "print(\"\\nContent of the output file:\")\n",
    "!head -10 \"outputs/movie_name_count_views\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTauFvnvlF1c"
   },
   "source": [
    "##Next\n",
    "The next [colab](https://github.com/OpenMined/PipelineDP/blob/main/docs/tutorial_1/2_beam_laplace_mechansim.ipynb) introduces Laplace mechanism and how to use it."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1_beam_introduction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
