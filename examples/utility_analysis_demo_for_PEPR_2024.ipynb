{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/OpenMined/PipelineDP/blob/main/utility_analysis/examples/Utility_Analysis_for_PipelineDP.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/OpenMined/PipelineDP/blob/main/utility_analysis/examples/Utility_Analysis_for_PipelineDP.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "eBRE5HwywpEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background"
      ],
      "metadata": {
        "id": "FYfEMiaaMRsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About this colab\n",
        "\n",
        "This colab demontrates the Utility Analysis functionality of PipelineDP.\n",
        "\n",
        "In brief, the analysis aims to answer two questions:\n",
        "1. **Tuning**: What are good *hyper parameters* for a differentially-private (DP) pipeline?\n",
        "1. **Analysis**: What is the *error* introduced by DP processing for these hyper parameters?\n",
        "\n",
        "**Disclaimers:**\n",
        "* the result of the analysis itself are not differentially-private, which means they need to be handled with care and, normally, they should not be shared broadly.\n",
        "* this colab was only tested with Google Colab. You can open this colab in Google Colab with by clicking the button in the top."
      ],
      "metadata": {
        "id": "vpq9D3YjdD5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audience\n",
        "The colab aims to be self-contained. However, the codelab mainly targets users who are familiar with basic knowledge of [Differential Privacy](https://en.wikipedia.org/wiki/Differential_privacy) (DP) and have one of the following goals:\n",
        "\n",
        "1. Understand how DP affects the result.\n",
        "1. Tune DP parameters to get the best data utility for a given level of user privacy.\n",
        "\n",
        "We recommend all readers to take a look at the Background section of the PipelineDP [restaurant visits example]((https://github.com/OpenMined/PipelineDP/blob/main/examples/restaurant_visits.ipynb). It contains the key definitions used across this colab."
      ],
      "metadata": {
        "id": "EN0N2M39wA9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Analysis\n",
        "\n",
        "The quality of differentially-private output might deteriorate due to several factors and depends on several parameters.\n",
        "\n",
        "In this Colab we consider two main factors:\n",
        "1. Adding noise (e.g. using [Laplace mechanism](https://en.wikipedia.org/wiki/Additive_noise_differential_privacy_mechanisms))\n",
        "1. Limiting maximum contribution of a single individual\n",
        "\n",
        "The scale of the error introduced by noise is well-known and independent of the data. The effect of contribution bounding is more complicated.\n",
        "\n",
        "## Why contribution bounding is required\n",
        "\n",
        "[Laplace and Gaussian mechanisms](https://en.wikipedia.org/wiki/Additive_noise_differential_privacy_mechanisms) require that adding one individual to the input dataset has limited effect on the output. The maximum change is called *sensitivity*.\n",
        "\n",
        "## Why contribution bounding is complicated\n",
        "\n",
        "In a real dataset,  one inidividual can often contribute arbitrary large number of data records. In order to limit sensitivity, PipelineDP performs contribution bounding by sampling records per user, which leads to dropping some data. The error introduced by this procedure depends on the input data.\n",
        "\n",
        "This presents a **bias-variance tradeoff**:\n",
        "\n",
        "1. Larger contribution bounding parameters lead to more data kept but larger noise error\n",
        "1. Smaller contribution bounding parameters lead to less data kept, smaller noise error.\n",
        "\n",
        "Contribution bounding error intoduces bias, while Laplace (or Gaussian) noise introduces variance.\n",
        "\n",
        "## What PipelineDP analysis does\n",
        "\n",
        "The analysis has two parts:\n",
        "\n",
        "1. **Utility Analysis** computes the error introduced by DP processing for a fixed dataset and hyper-parameters;\n",
        "1. **Parameter tuning** recommends hyper-parameters to make error smaller using utility analysis under the hood.\n",
        "\n",
        "\n",
        "**Note:** The recommended parameters would not necessarily be optimal. Since the analysis is designed for large distributed datasets and can be expensive, the goal is to find hyper-parameters that are \"good enough\" but not necessarily optimal.\n",
        "\n"
      ],
      "metadata": {
        "id": "T9816-txVJp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code\n",
        "\n",
        "This colab uses a fake dataset of restaurant visits. Here's the plan:\n",
        "\n",
        "1. Install dependencies and download data;\n",
        "1. Load the data into a Pandas DataFrame; feel free to play with the data;\n",
        "1. Demonstrate how to apply DP with PipelineDP. Show comparison of the raw and differentially-private counts.\n",
        "1. Show how to use Utility Analysis to find good hyper parameters.\n",
        "\n",
        "To perform these steps, run the cells below.\n"
      ],
      "metadata": {
        "id": "ffRy7I0lNLkT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPQecaArWpFm",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install dependencies and download data\n",
        "# !pip install pipeline-dp==0.2.2rc2\n",
        "!git clone https://github.com/OpenMined/PipelineDP.git\n",
        "!pip install python-dp~=1.1.5rc4\n",
        "import sys\n",
        "sys.path.append(\"PipelineDP\")\n",
        "\n",
        "#Download restaurant dataset from github\n",
        "!wget https://raw.githubusercontent.com/google/differential-privacy/main/examples/go/data/week_data.csv\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import pipeline_dp\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and inspect the data\n",
        "\n",
        "The dataset used in this Colab is a simulated dataset of visits to some restaurant during a 7-day period. The code below loads the dataset in a DataFrame and prints some of its records."
      ],
      "metadata": {
        "id": "LSjRXvj_f4vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Read data\n",
        "df = pd.read_csv('week_data.csv')\n",
        "df.rename(inplace=True, columns={'VisitorId' : 'user_id', 'Time entered' : 'enter_time', 'Time spent (minutes)' : 'spent_minutes', 'Money spent (euros)' : 'spent_money', 'Day' : 'day'})\n",
        "data = [index_row[1] for index_row in df.iterrows()]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "-yw9Y0S99RnI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal is to compute the number of visits for each day of the week in a differentially-private manner. In PipelineDP terminology, a \"day of week\" represents a data *partition*.\n",
        "\n",
        "Contribution bounding paramers:\n",
        "\n",
        "- **max_partitions_contributed** : maximum days per week one visitor can eats in a restaurant\n",
        "\n",
        "- **max_contributions_per_partition**: maximum times per day can a visitor eat in a restaurant\n"
      ],
      "metadata": {
        "id": "x7wSPPOzW52c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing DP count"
      ],
      "metadata": {
        "id": "AF4EWeapPHpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title DP parameters and Hyper parameters\n",
        "@dataclass\n",
        "class DPBudget:\n",
        "  epsilon: float\n",
        "  delta: float = 0"
      ],
      "metadata": {
        "id": "GY1ZfERqPVzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hyper parameters\n",
        "@dataclass\n",
        "class HyperParameters:\n",
        "  noise_kind: pipeline_dp.NoiseKind # Laplace or Gaussian\n",
        "  max_partitions_contributed: int\n",
        "  max_contributions_per_partition: int"
      ],
      "metadata": {
        "id": "_l_B6wOcQu27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Function compute_counts_with_dp and visualising (code is long, can be skipped)\n",
        "\n",
        "def get_data_extractors():\n",
        "  # Define privacy ID, partition key and aggregated value extractors.\n",
        "  # The aggregated value extractor isn't used in this example.\n",
        "  return pipeline_dp.DataExtractors(\n",
        "    partition_extractor=lambda row: row.day,\n",
        "    privacy_id_extractor=lambda row: row.user_id,\n",
        "    value_extractor=lambda row: 1)\n",
        "\n",
        "def compute_counts_with_dp(rows:list, budget: DPBudget, params: HyperParameters):\n",
        "  # Set the backend to local backend. Other options (Beam or Spark)\n",
        "  # are possible.\n",
        "  backend = pipeline_dp.LocalBackend()\n",
        "\n",
        "  # Define the total budget.\n",
        "  budget_accountant = pipeline_dp.NaiveBudgetAccountant(total_epsilon=budget.epsilon, total_delta=budget.delta)\n",
        "\n",
        "  # Create DPEngine which will execute the logic.\n",
        "  dp_engine = pipeline_dp.DPEngine(budget_accountant, backend)\n",
        "\n",
        "  data_extractors = get_data_extractors()\n",
        "  # Configure the aggregation parameters.\n",
        "  params = pipeline_dp.AggregateParams(\n",
        "    noise_kind=params.noise_kind,\n",
        "    # This example computes only count but we can compute multiple\n",
        "    # ... metrics at once.\n",
        "    metrics=[pipeline_dp.Metrics.COUNT],\n",
        "    # Limits visits contributed by a visitor. A visitor can contribute to\n",
        "    # ... up to 3 days\n",
        "    max_partitions_contributed=params.max_partitions_contributed,\n",
        "    # ... and up to 2 visits per day.\n",
        "    max_contributions_per_partition=params.max_contributions_per_partition)\n",
        "  # Configure the output partition keys as they are publicly known.\n",
        "  # The output should include all week days.\n",
        "  public_partitions=list(range(1, 8))\n",
        "\n",
        "  # Create a computational graph for the aggregation.\n",
        "  # All computations are lazy. dp_result is iterable, but iterating it would\n",
        "  # fail until budget is computed (below).\n",
        "  # Itâ€™s possible to call DPEngine.aggregate multiple times with different\n",
        "  # metrics to compute.\n",
        "  dp_result = dp_engine.aggregate(rows, params, data_extractors, public_partitions)\n",
        "\n",
        "  # Compute budget per each DP operation.\n",
        "  budget_accountant.compute_budgets()\n",
        "\n",
        "  # Here's where the lazy iterator initiates computations and gets transformed\n",
        "  # into actual results\n",
        "  return list(dp_result)\n",
        "\n",
        "def plot_comparison(dp_result):\n",
        "  non_dp_count = [0] * 7\n",
        "  days = range(1, 7)\n",
        "  for row in data:\n",
        "    index = row['day'] - 1\n",
        "    non_dp_count[index] += 1\n",
        "\n",
        "  # Copy the DP result to a list\n",
        "  dp_count = [0] * 7\n",
        "  for count_sum_per_day in dp_result:\n",
        "    index =  count_sum_per_day[0] - 1\n",
        "    dp_count[index] = count_sum_per_day[1][0]\n",
        "\n",
        "  days = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
        "  x = np.arange(len(days))\n",
        "\n",
        "  width = 0.35\n",
        "  fig, ax = plt.subplots()\n",
        "  rects1 = ax.bar(x - width/2, non_dp_count, width, label='non-DP')\n",
        "  rects2 = ax.bar(x + width/2, dp_count, width, label='DP')\n",
        "  ax.set_ylabel('Visit count')\n",
        "  ax.set_title('Count visits per day')\n",
        "  ax.set_xticks(x)\n",
        "  ax.set_xticklabels(days)\n",
        "  ax.legend()\n",
        "  fig.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "2knIZRf_CSXm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's set some DP budget\n",
        "dp_budget = DPBudget(epsilon=1.0, delta=1e-8)"
      ],
      "metadata": {
        "id": "RzfZo7j-Q6Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us try with different hyper-parameters (not the output is randomized)\n",
        "hyper_params = HyperParameters(noise_kind = pipeline_dp.NoiseKind.LAPLACE,\n",
        "                               max_partitions_contributed=1,\n",
        "                               max_contributions_per_partition=1)\n",
        "\n",
        "plot_comparison(compute_counts_with_dp(data, dp_budget, hyper_params))"
      ],
      "metadata": {
        "id": "5X8M66DfRJxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DP counts are way smaller than the actual counts. This suggests a lot of data records are dropped because of aggressive contribution bounding.\n",
        "\n",
        "Let's try larger paramers:"
      ],
      "metadata": {
        "id": "HLdW11gnSUm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyper_params = HyperParameters(noise_kind = pipeline_dp.NoiseKind.LAPLACE,\n",
        "                               max_partitions_contributed=10,\n",
        "                               max_contributions_per_partition=10)\n",
        "\n",
        "plot_comparison(compute_counts_with_dp(data, dp_budget, hyper_params))"
      ],
      "metadata": {
        "id": "VVIXayELSm3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DP counts are closer to actual counts, but maybe we can do better. Let's use the analysis functionality for that!"
      ],
      "metadata": {
        "id": "Yowm7UpNSw8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PipelineDP Analysis"
      ],
      "metadata": {
        "id": "itBi6J70S9X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title run_analysis function (unfold to see implementation details)\n",
        "import analysis\n",
        "from analysis import parameter_tuning\n",
        "from pipeline_dp.dataset_histograms import computing_histograms\n",
        "\n",
        "@dataclass\n",
        "class AnalysisResult:\n",
        "  recommended_hyper_params: HyperParameters\n",
        "  average_rmse_per_partition: float\n",
        "\n",
        "def run_analysis(rows, budget: DPBudget):\n",
        "  # We now that output keys are from 1 to 7 (Mon-Sun). Let us set them as\n",
        "  # public partitions\n",
        "  public_partitions=list(range(1, 8))\n",
        "\n",
        "  # backend and data_extractors specified the same as for DP count.\n",
        "  backend = pipeline_dp.LocalBackend()\n",
        "  data_extractors = get_data_extractors()\n",
        "\n",
        "  # At first compute contirbution_histograms, which contains some statistics\n",
        "  # which are used by parameter tuning for finding hyper-parameters candidates.\n",
        "  contribution_histograms = computing_histograms.compute_dataset_histograms(\n",
        "            rows, data_extractors, backend)\n",
        "  contribution_histograms = list(contribution_histograms)[0]\n",
        "\n",
        "  # Specify parameters for tuning\n",
        "\n",
        "  # For now only optimizing Absolute error is supported.\n",
        "  minimizing_function = parameter_tuning.MinimizingFunction.ABSOLUTE_ERROR\n",
        "\n",
        "  # Which contribution bounding parameters to tune.\n",
        "  parameters_to_tune = parameter_tuning.ParametersToTune(\n",
        "        max_partitions_contributed=True, max_contributions_per_partition=True)\n",
        "\n",
        "  aggregate_params = pipeline_dp.AggregateParams(\n",
        "        noise_kind=pipeline_dp.NoiseKind.LAPLACE, # not used by tuning, required by PipelineDP validation\n",
        "        metrics=[pipeline_dp.Metrics.COUNT],\n",
        "        max_partitions_contributed=1, # not used by tuning, required by PipelineDP validation\n",
        "        max_contributions_per_partition=1 # not used by tuning, required by PipelineDP validation\n",
        "  )\n",
        "  tune_options = parameter_tuning.TuneOptions(\n",
        "        epsilon=budget.epsilon,\n",
        "        delta=budget.delta,\n",
        "        aggregate_params=aggregate_params,\n",
        "        function_to_minimize=minimizing_function,\n",
        "        parameters_to_tune=parameters_to_tune)\n",
        "\n",
        "  result, _ = parameter_tuning.tune(rows, backend, contribution_histograms,\n",
        "                                                  tune_options, data_extractors,\n",
        "                                                  public_partitions)\n",
        "  result = list(result)[0] # result is lazy iterator, converting to list enforce run.\n",
        "  index_best = result.index_best\n",
        "\n",
        "  # Extract the recommended parameters.\n",
        "  # Result contains information about all hyper-parameters that were tried.\n",
        "  noise_kind = result.utility_analysis_parameters.noise_kind[index_best]\n",
        "  max_partitions_contributed = result.utility_analysis_parameters.max_partitions_contributed[index_best]\n",
        "  max_contributions_per_partition = result.utility_analysis_parameters.max_contributions_per_partition[index_best]\n",
        "\n",
        "  # Extract utility information\n",
        "  utility_report_for_best_parameters = result.utility_reports[index_best]\n",
        "  rmse_for_best_parameters = utility_report_for_best_parameters.metric_errors[0].absolute_error.rmse\n",
        "  clear_output()\n",
        "  return AnalysisResult(HyperParameters(noise_kind, max_partitions_contributed, max_contributions_per_partition), average_rmse_per_partition=rmse_for_best_parameters)"
      ],
      "metadata": {
        "id": "YDKtVoQdInS3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run analysis\n",
        "recommended_hyper_parameters = run_analysis(data, dp_budget).recommended_hyper_params\n",
        "\n",
        "print(\"Recommended parameters:\")\n",
        "print(f\"  noise={recommended_hyper_parameters.noise_kind.value}\")\n",
        "print(f\"  max_partitions_contributed={recommended_hyper_parameters.max_partitions_contributed}\")\n",
        "print(f\"  max_contributions_per_partition={recommended_hyper_parameters.max_contributions_per_partition}\")"
      ],
      "metadata": {
        "id": "_ltofYo2UrbM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff3970ac-f04b-45d7-86a2-b64d02cb9ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommended parameters:\n",
            "  noise=laplace\n",
            "  max_partitions_contributed=4\n",
            "  max_contributions_per_partition=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try recommended parameters\n",
        "plot_comparison(compute_counts_with_dp(data, dp_budget, recommended_hyper_parameters))"
      ],
      "metadata": {
        "id": "FrBGRe1SU3eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DP counts are now much closer to actual counts! If you like to play with different parameters further, you can do so in the next cell."
      ],
      "metadata": {
        "id": "TlUghb52VFRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Inspect different paramters\n",
        "noise = \"Laplace\" # @param [\"Laplace\", \"Gaussian\"]\n",
        "max_partitions_contributed = 1 #@param { type: \"number\" }\n",
        "max_contributions_per_partition = 1 #@param { type: \"number\" }\n",
        "\n",
        "hyper_params = HyperParameters(noise_kind = pipeline_dp.NoiseKind.LAPLACE,\n",
        "                               max_partitions_contributed=max_partitions_contributed,\n",
        "                               max_contributions_per_partition=max_contributions_per_partition)\n",
        "\n",
        "plot_comparison(compute_counts_with_dp(data, dp_budget, hyper_params))"
      ],
      "metadata": {
        "id": "JNl293mjBZwF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j9jb8Y_NGs9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "We showed how to use PipelineDP to perform differentially-private statistics, understand the data quality, and improve it by choosing better hyper-parameters.\n",
        "\n",
        "**Go bigger.** PipelineDP shines with large, distributed, many terrabytes-sized datasets. Since the example dataset was very small, we used the local (in-memory) computation. For much larger datasets, you can use PipelineDP to perform calculations on top of Apache Spark or Apache Beam. The complexity is abstracted away, so the main code wouldn't be that much different.\n",
        "\n",
        "If you like to try PipelineDP analysis on a large distributed dataset, feel free to contact us by dp-open-source@google.com. We're happy to help!"
      ],
      "metadata": {
        "id": "7VYPbWBZV11u"
      }
    }
  ]
}