{
	"cells": [
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Using PipelineDP with Apache Spark on AWS Glue\n",
				"\n",
				"This notebook can be used in AWS Glue to run a job.\n",
				"\n",
				"To create the job, you can follow the steps [here](https://catalog.us-east-1.prod.workshops.aws/workshops/ee59d21b-4cb8-4b3d-a629-24537cf37bb5/en-US/lab3/etl-job), but uploading this notebook instead of the one they mention.\n",
				"\n",
				"Make sure to also create a IAM role and add the correct permissions. See how to do this [here](https://docs.aws.amazon.com/glue/latest/ug/notebook-getting-started.html#create-notebook-permissions).\n",
				"\n",
				"Finally, the file we use is on S3. So remember to add the files you want to handle to S3 as well."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true
			},
			"outputs": [],
			"source": [
				"%glue_version 3.0"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true
			},
			"outputs": [],
			"source": [
				"%additional_python_modules pipeline_dp"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true
			},
			"outputs": [],
			"source": [
				"print(\"\\nSTARTED\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true
			},
			"outputs": [],
			"source": [
				"import pipeline_dp"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true
			},
			"outputs": [],
			"source": [
				"from dataclasses import dataclass\n",
				"\n",
				"@dataclass\n",
				"class MovieView:\n",
				"    user_id: int\n",
				"    movie_id: int\n",
				"    rating: int\n",
				"\n",
				"\n",
				"def parse_line(line, movie_id):\n",
				"    # 'line' has format \"user_id,rating,date\"\n",
				"    split_parts = line.split(',')\n",
				"    user_id = int(split_parts[0])\n",
				"    rating = int(split_parts[1])\n",
				"    return MovieView(user_id, movie_id, rating)\n",
				"\n",
				"def parse_partition(iterator):\n",
				"    movie_id = None\n",
				"    for line in iterator:\n",
				"        if line[-1] == ':':\n",
				"            # 'line' has a format \"movie_id:'\n",
				"            movie_id = int(line[:-1])\n",
				"        else:\n",
				"            # 'line' has a format \"user_id,rating,date\"\n",
				"            yield parse_line(line, movie_id)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# REPLACE IT WITH YOURS\n",
				"\n",
				"BUCKET_FILE = \"s3://bucketpipelinedp/sample_combined_data_1.txt\""
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true
			},
			"outputs": [],
			"source": [
				"from awsglue.transforms import *\n",
				"from pyspark.context import SparkContext\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"  \n",
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"job = Job(glueContext)\n",
				"\n",
				"movie_views = sc.textFile(BUCKET_FILE).mapPartitions(parse_partition)\n",
				"\n",
				"backend = pipeline_dp.SparkRDDBackend(glueContext)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true
			},
			"outputs": [],
			"source": [
				"import pipeline_dp\n",
				"from pipeline_dp.private_spark import make_private\n",
				"\n",
				"\n",
				"# Define the privacy budget available for our computation.\n",
				"budget_accountant = pipeline_dp.NaiveBudgetAccountant(total_epsilon=1,\n",
				"                                                      total_delta=1e-6)\n",
				"\n",
				"# Create a DPEngine instance.\n",
				"dp_engine = pipeline_dp.DPEngine(budget_accountant, backend)\n",
				"\n",
				"# Wrap Spark's RDD into its private version\n",
				"private_movie_views = \\\n",
				"   make_private(movie_views, budget_accountant, lambda mv: mv.user_id)\n",
				"\n",
				"params = pipeline_dp.AggregateParams(\n",
				"   noise_kind=pipeline_dp.NoiseKind.LAPLACE,\n",
				"   metrics=[\n",
				"      pipeline_dp.Metrics.COUNT, pipeline_dp.Metrics.SUM,\n",
				"      pipeline_dp.Metrics.MEAN, pipeline_dp.Metrics.VARIANCE\n",
				"   ],\n",
				"   # Limits to how much one user can contribute:\n",
				"   # .. at most two movies rated per user\n",
				"   max_partitions_contributed=2,\n",
				"   # .. at most one rating for each movie\n",
				"   max_contributions_per_partition=1,\n",
				"   # .. with minimal rating of \"1\"\n",
				"   min_value=1,\n",
				"   # .. and maximum rating of \"5\"\n",
				"   max_value=5,\n",
				"   contribution_bounds_already_enforced=True)\n",
				"\n",
				"# Specify how to extract privacy_id and value from an\n",
				"# element of movie view collection.\n",
				"data_extractors = pipeline_dp.DataExtractors(\n",
				"   # The aggregation key: we're grouping by movies\n",
				"   partition_extractor=lambda mv: mv.movie_id,\n",
				"   # The value we're aggregating: we're summing up ratings\n",
				"   value_extractor=lambda mv: mv.rating)\n",
				"\n",
				"# Run aggregation.\n",
				"dp_result = dp_engine.aggregate(movie_views, params, data_extractors)\n",
				"\n",
				"budget_accountant.compute_budgets()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true
			},
			"outputs": [],
			"source": [
				"dp_result = dp_result.collect()\n",
				"dp_result"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.7.15"
		},
		"vscode": {
			"interpreter": {
				"hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
			}
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
